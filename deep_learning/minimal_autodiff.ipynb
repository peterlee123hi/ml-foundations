{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328e4647",
   "metadata": {},
   "source": [
    "# Miniminal Autodiff Engine\n",
    "\n",
    "This notebook contains a minimal implementation of an automatic differentiation engine that supports reverse-mode autodiff on N-dimensional tensors:\n",
    "- Tensor: core data structure and computation graph\n",
    "- Optimizers: update model parameters using gradients (SGD, SGDMomentum, Adam)\n",
    "- MLP module: neural network library with linear layers and activations\n",
    "- Loss and training functions: measure prediction error and run training loop\n",
    "\n",
    "Note: this project is self-contained and won't be shared with CNN / Transformer / Diffusion, those projects will use Pytorch autograd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc48969",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "a0451c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)\n",
    "np.random.seed(0)\n",
    "\n",
    "def _unbroadcast(grad, target_shape):\n",
    "    \"\"\"\n",
    "    Reduce a gradient to match the given target shape.\n",
    "\n",
    "    Params:\n",
    "        grad: np.array, the gradient with the broadcasted shape\n",
    "        target_shape: tuple of int, the shape of the tensor before broadcasting\n",
    "\n",
    "    Returns:\n",
    "        result: np.array, the gradient with reduced dimensions and summed so it matches the target shape\n",
    "    \"\"\"\n",
    "    # grad.ndim is number of axes, len(target_shape) is desired number of axes.\n",
    "    # For [[a, b], [c, d]], the result of sum(axis=0) is [a+c, b+d]\n",
    "    while grad.ndim > len(target_shape):\n",
    "        grad = grad.sum(axis=0)\n",
    "    \n",
    "    # For every axis where the original tensor had size 1 and was broadcasted, we sum\n",
    "    # the gradient along that axis to collapse it back.\n",
    "    for axis, dim in reversed(list(enumerate(target_shape))):\n",
    "        if dim == 1:\n",
    "            grad = grad.sum(axis=axis, keepdims=True)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfed5d6",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "The `Tensor` is a core data structure used to construct computation graphs or neural networks that process input data, generate predictions, and evaluate a loss function. It also enables us to perform backpropagation to compute gradients at each node so the network's weights can be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "41ac9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=False):\n",
    "        # Numpy scalar, vector, matrix\n",
    "        self.data = np.array(data, dtype=np.float64)\n",
    "\n",
    "        # Determines if we should store gradient\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        # Gradient of the loss wrt this tensor (same shape as .data)\n",
    "        self.grad = None\n",
    "\n",
    "        # Closure that computes local gradients and accumulates them into parent tensors\n",
    "        self._backward = lambda: None\n",
    "\n",
    "        # Parent tensors\n",
    "        self._prev = ()\n",
    "\n",
    "        # Op name for logging\n",
    "        self._op = \"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        # String representation for logging\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad}, op={self._op})\"\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Performs backpropagation on the computation graph and updates internal variables.\"\"\"\n",
    "        if self.data.shape != ():\n",
    "            raise ValueError(\"backward() assumes scalar output on the loss layer\")\n",
    "\n",
    "        # Set loss gradient to ones\n",
    "        self.grad = np.array(1.0, dtype=np.float64)\n",
    "\n",
    "        # Build DFS topological sort of computation graph\n",
    "        topo, visited = [], set()\n",
    "        def build(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for p in v._prev:\n",
    "                    build(p)\n",
    "                topo.append(v)\n",
    "        build(self)\n",
    "\n",
    "        # Reverse topo sort order (starting from the outermost / loss layer)\n",
    "        for v in reversed(topo):\n",
    "            if v.grad is not None:\n",
    "                v._backward()\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"Elementwise addition with broadcasting, returns a new Tensor and sets up gradient flow to both parents\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "\n",
    "        # Construct new tensor resulting from the operation\n",
    "        output_data = self.data + other.data\n",
    "        output = Tensor(data=output_data, requires_grad=(self.requires_grad or other.requires_grad))\n",
    "        output._prev = (self, other)\n",
    "        output._op = \"add\"\n",
    "\n",
    "        # For the new tensor, set backprop method to update the gradients of its parents.\n",
    "        # For addition, the local derivative with respect to each parent is 1, so the upstream\n",
    "        # gradient is passed straight through.\n",
    "        # .grad has the gradient with respect to the outermost layer.\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = _unbroadcast(output.grad, self.data.shape)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "            if other.requires_grad:\n",
    "                grad = _unbroadcast(output.grad, other.data.shape)\n",
    "                other.grad = grad if other.grad is None else other.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Elementwise multiplication with broadcasting, returns a new Tensor and sets up gradient flow to both parents\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "\n",
    "        output_data = self.data * other.data\n",
    "        output = Tensor(data=output_data, requires_grad=(self.requires_grad or other.requires_grad))\n",
    "        output._prev = (self, other)\n",
    "        output._op = \"mul\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = output.grad * other.data\n",
    "                grad = _unbroadcast(grad, self.data.shape)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "            if other.requires_grad:\n",
    "                grad = output.grad * self.data\n",
    "                grad = _unbroadcast(grad, other.data.shape)\n",
    "                other.grad = grad if other.grad is None else other.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        \"\"\"Reverse multiplication operator, uses predefined ops\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        return other * self\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"Matrix multiplication (@ operator), returns a new Tensor and sets up gradient flow to both parents\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "\n",
    "        output_data = self.data @ other.data\n",
    "        output = Tensor(data=output_data, requires_grad=(self.requires_grad or other.requires_grad))\n",
    "        output._prev = (self, other)\n",
    "        output._op = \"matmul\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if other.data.ndim == 1:\n",
    "                    grad = np.outer(output.grad, other.data)\n",
    "                else:\n",
    "                    grad = output.grad @ other.data.T\n",
    "                grad = _unbroadcast(grad, self.data.shape)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "            if other.requires_grad:\n",
    "                if self.data.ndim == 1:\n",
    "                    grad = np.outer(self.data, output.grad)\n",
    "                else:\n",
    "                    grad = self.data.T @ output.grad\n",
    "                grad = _unbroadcast(grad, other.data.shape)\n",
    "                other.grad = grad if other.grad is None else other.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __rmatmul__(self, other):\n",
    "        \"\"\"Reverse matrix multiplication, uses predefined ops\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        return other.__matmul__(self)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negative operator, uses predefined ops\"\"\"\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        \"\"\"Negative operator, uses predefined ops\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        \"\"\"Reverse subtraction operator, uses predefined ops\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        return other + (-self)\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        \"\"\"Power (** operator), returns a new Tensor and sets up gradient flow to both parents\"\"\"\n",
    "        assert isinstance(power, (int, float)), \"only scalar powers supported\"\n",
    "        output_data = self.data ** power\n",
    "        output = Tensor(output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"pow\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = output.grad * (power * (self.data ** (power - 1)))\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        \"\"\"True division, uses predefined ops\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        return self * (other ** -1)\n",
    "\n",
    "    def exp(self):\n",
    "        \"\"\"e^x, returns a new Tensor and sets up gradient flow to both parents\"\"\"\n",
    "        output_data = np.exp(self.data)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"exp\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = output.grad * output.data\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def log(self):\n",
    "        \"\"\"Natural logarithm, returns a new Tensor and sets up gradient flow to the parent\"\"\"\n",
    "        output_data = np.log(self.data)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"log\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = output.grad * (1.0 / self.data)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def relu(self):\n",
    "        \"\"\"ReLU activation, returns a new Tensor and sets up gradient flow to the parent\"\"\"\n",
    "        output_data = np.maximum(0, self.data)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"relu\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = output.grad * (self.data > 0).astype(self.data.dtype)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def sigmoid(self):\n",
    "        \"\"\"Sigmoid activation, returns a new Tensor and sets up gradient flow to the parent\"\"\"\n",
    "        output_data = 1 / (1 + np.exp(-self.data))\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"sigmoid\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = output.grad * (output.data * (1 - output.data))\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def tanh(self):\n",
    "        \"\"\"Tanh activation, returns a new Tensor and sets up gradient flow to the parent\"\"\"\n",
    "        output_data = np.tanh(self.data)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"tanh\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = output.grad * (1 - output.data ** 2)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def sum(self, axis=None, keepdims=False):\n",
    "        \"\"\"Sum of tensor elements over given axes, returns a new Tensor\"\"\"\n",
    "        output_data = self.data.sum(axis=axis, keepdims=keepdims)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"sum\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = output.grad\n",
    "                if not keepdims and axis is not None:\n",
    "                    grad = np.expand_dims(grad, axis=axis)\n",
    "                grad = np.broadcast_to(grad, self.data.shape)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def mean(self, axis=None, keepdims=False):\n",
    "        \"\"\"Mean of tensor elements over given axes, returns a new Tensor\"\"\"\n",
    "        output_data = self.data.mean(axis=axis, keepdims=keepdims)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"mean\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = output.grad\n",
    "                if axis is None:\n",
    "                    denom = self.data.size\n",
    "                else:\n",
    "                    axes = (axis,) if isinstance(axis, int) else axis\n",
    "                    denom = 1\n",
    "                    for a in axes:\n",
    "                        denom *= self.data.shape[a]\n",
    "                    if not keepdims:\n",
    "                        grad = np.expand_dims(grad, axis=axis)\n",
    "\n",
    "                grad = np.ones_like(self.data) * (grad / denom)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def max(self, axis=None, keepdims=False):\n",
    "        \"\"\"Max of tensor elements over given axes, returns a new Tensor\"\"\"\n",
    "        output_data = self.data.max(axis=axis, keepdims=keepdims)\n",
    "        output = Tensor(output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"max\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                # Mask out the maxima\n",
    "                mask = (self.data == output.data)\n",
    "                grad = mask * output.grad\n",
    "                if not keepdims and axis is not None:\n",
    "                    grad = np.expand_dims(grad, axis=axis)\n",
    "                grad = _unbroadcast(grad, self.data.shape)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def reshape(self, *shape):\n",
    "        \"\"\"Reshape tensor to given shape, returns a new Tensor\"\"\"\n",
    "        output_data = self.data.reshape(*shape)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"reshape\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = output.grad.reshape(self.data.shape)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def transpose(self, *axes):\n",
    "        \"\"\"Transpose tensor according to given axes, returns a new Tensor\"\"\"\n",
    "        output_data = self.data.transpose(*axes)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"transpose\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if axes:\n",
    "                    inv_axes = np.argsort(axes)\n",
    "                    grad = output.grad.transpose(*inv_axes)\n",
    "                else:\n",
    "                    grad = output.grad.transpose()\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def broadcast_to(self, shape):\n",
    "        \"\"\"Broadcast tensor to given shape, returns a new Tensor\"\"\"\n",
    "        output_data = np.broadcast_to(self.data, shape)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"broadcast_to\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = _unbroadcast(output.grad, self.data.shape)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Slice operation ([...] to retrieve), returns a new Tensor\"\"\"\n",
    "        output_data = self.data[idx]\n",
    "        output = Tensor(output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"slice\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = np.zeros_like(self.data)\n",
    "                np.add.at(grad, idx, output.grad)\n",
    "                self.grad = grad if self.grad is None else self.grad + grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133170c",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "6b378647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: Tensor(data=[1. 2.], grad=[0. 2.], op=)\n",
      "W: Tensor(data=[[1. 2.]\n",
      " [1. 1.]], grad=[[ 2.  4.]\n",
      " [-2. -4.]], op=)\n",
      "b: Tensor(data=[1. 1.], grad=[ 2. -2.], op=)\n",
      "y: Tensor(data=[6. 4.], grad=[ 2. -2.], op=add)\n",
      "label: Tensor(data=[5. 5.], grad=None, op=)\n",
      "loss: Tensor(data=2.0, grad=1.0, op=sum)\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([1, 2], requires_grad=True)\n",
    "W = Tensor([[1, 2], [1, 1]], requires_grad=True)\n",
    "b = Tensor([1, 1], requires_grad=True)\n",
    "y = W @ x + b\n",
    "label = Tensor([5, 5])\n",
    "loss = ((y - label) ** 2).sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"W:\", W)\n",
    "print(\"b:\", b)\n",
    "print(\"y:\", y)\n",
    "print(\"label:\", label)\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "da4e1672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add: [1.] [1.] (expected 1, 1)\n",
      "mul: [4.] [3.] (expected 4, 3)\n",
      "pow: [12.] (expected 12)\n",
      "div: [0.5] [-1.5] (expected 1/2, -6/4 = -1.5)\n",
      "exp: [7.389] (expected 7.389)\n",
      "log: [0.5] (expected 0.5)\n",
      "relu: [0. 1.] (expected [0, 1])\n",
      "sigmoid: [0.25] (expected 0.25)\n",
      "tanh: [1.] (expected 1.0)\n",
      "matmul: [[3. 4.]] [1. 2.] (expected [[3, 4]], [1, 2])\n",
      "sum: [1. 1. 1.] (expected [1, 1, 1])\n",
      "mean: [0.333 0.333 0.333] (expected [1/3, 1/3, 1/3])\n",
      "broadcast: [[3. 3.]] (expected [[3, 3]])\n",
      "reshape+transpose: [[1. 1.]\n",
      " [1. 1.]] (expected all ones)\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([3.0], requires_grad=True)\n",
    "b = Tensor([4.0], requires_grad=True)\n",
    "c = (a + b).sum()\n",
    "c.backward()\n",
    "print(\"add:\", a.grad, b.grad, \"(expected 1, 1)\")\n",
    "\n",
    "a = Tensor([3.0], requires_grad=True)\n",
    "b = Tensor([4.0], requires_grad=True)\n",
    "c = (a * b).sum()\n",
    "c.backward()\n",
    "print(\"mul:\", a.grad, b.grad, \"(expected 4, 3)\")\n",
    "\n",
    "x = Tensor([2.0], requires_grad=True)\n",
    "y = (x ** 3).sum()\n",
    "y.backward()\n",
    "print(\"pow:\", x.grad, \"(expected 12)\")\n",
    "\n",
    "a = Tensor([6.0], requires_grad=True)\n",
    "b = Tensor([2.0], requires_grad=True)\n",
    "c = (a / b).sum()\n",
    "c.backward()\n",
    "print(\"div:\", a.grad, b.grad, \"(expected 1/2, -6/4 = -1.5)\")\n",
    "\n",
    "x = Tensor([2.0], requires_grad=True)\n",
    "y = x.exp().sum()\n",
    "y.backward()\n",
    "print(\"exp:\", x.grad, \"(expected 7.389)\")\n",
    "\n",
    "x = Tensor([2.0], requires_grad=True)\n",
    "y = x.log().sum()\n",
    "y.backward()\n",
    "print(\"log:\", x.grad, \"(expected 0.5)\")\n",
    "\n",
    "x = Tensor([-1.0, 2.0], requires_grad=True)\n",
    "y = x.relu().sum()\n",
    "y.backward()\n",
    "print(\"relu:\", x.grad, \"(expected [0, 1])\")\n",
    "\n",
    "x = Tensor([0.0], requires_grad=True)\n",
    "y = x.sigmoid().sum()\n",
    "y.backward()\n",
    "print(\"sigmoid:\", x.grad, \"(expected 0.25)\")\n",
    "\n",
    "x = Tensor([0.0], requires_grad=True)\n",
    "y = x.tanh().sum()\n",
    "y.backward()\n",
    "print(\"tanh:\", x.grad, \"(expected 1.0)\")\n",
    "\n",
    "W = Tensor([[1.0, 2.0]], requires_grad=True)\n",
    "x = Tensor([3.0, 4.0], requires_grad=True)\n",
    "y = (W @ x).sum()\n",
    "y.backward()\n",
    "print(\"matmul:\", W.grad, x.grad, \"(expected [[3, 4]], [1, 2])\")\n",
    "\n",
    "x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "print(\"sum:\", x.grad, \"(expected [1, 1, 1])\")\n",
    "\n",
    "x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x.mean()\n",
    "y.backward()\n",
    "print(\"mean:\", x.grad, \"(expected [1/3, 1/3, 1/3])\")\n",
    "\n",
    "x = Tensor([[1.0, 2.0]], requires_grad=True)\n",
    "y = x.broadcast_to((3,2))\n",
    "z = y.sum()\n",
    "z.backward()\n",
    "print(\"broadcast:\", x.grad, \"(expected [[3, 3]])\")\n",
    "\n",
    "x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "y = x.reshape(4,)\n",
    "z = y.transpose().sum()\n",
    "z.backward()\n",
    "print(\"reshape+transpose:\", x.grad, \"(expected all ones)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184502c",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "As we saw in numerical optimization, after performing backpropagation and computing the gradient at each `Tensor`, there are different strategies on how to update the parameters based on the gradient and learning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "d75dfe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"Optimizer for performing stochastic gradient descent\"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        self.params = [param for param in params if param.requires_grad]\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Update .data based on the .grad and learning parameters\"\"\"\n",
    "        for param in self.params:\n",
    "            if param is not None:\n",
    "                param.data = param.data - self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Resets the parameters given to the optimizer\"\"\"\n",
    "        for param in self.params:\n",
    "            param.grad = None\n",
    "\n",
    "\n",
    "class SGDMomentum:\n",
    "    \"\"\"Optimizer for performing stochastic gradient descent with momentum\"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-2, momentum=0.9):\n",
    "        self.params = [param for param in params if param.requires_grad]\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.velocities = [np.zeros_like(param.data) for param in self.params]\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Update .data based on the .grad and learning parameters\"\"\"\n",
    "        for idx, param in enumerate(self.params):\n",
    "            if param.grad is not None:\n",
    "                # Update velocities for next step\n",
    "                self.velocities[idx] = self.momentum * self.velocities[idx] - self.lr * param.grad\n",
    "                param.data = param.data + self.velocities[idx]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Resets the parameters given to the optimizer\"\"\"\n",
    "        for param in self.params:\n",
    "            param.grad = None\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    \"\"\"Optimizer for performing Adam with first and second moments\"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n",
    "        self.params = [param for param in params if param.requires_grad]\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.eps = eps\n",
    "        self.time = 0\n",
    "        self.momentum = [np.zeros_like(param.data) for param in self.params]\n",
    "        self.velocities = [np.zeros_like(param.data) for param in self.params]\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Update .data based on the .grad and learning parameters\"\"\"\n",
    "        self.time += 1\n",
    "        beta1, beta2 = self.betas\n",
    "        for idx, param in enumerate(self.params):\n",
    "            if param.grad is None:\n",
    "                continue\n",
    "            grad = param.grad\n",
    "\n",
    "            # Update first and second moment estimates\n",
    "            self.momentum[idx] = beta1 * self.momentum[idx] + (1 - beta1) * grad\n",
    "            self.velocities[idx] = beta2 * self.velocities[idx] + (1 - beta2) * (grad * grad)\n",
    "\n",
    "            # Correct bias in estimates\n",
    "            m_hat = self.momentum[idx] / (1 - beta1 ** self.time)\n",
    "            v_hat = self.velocities[idx] / (1 - beta2 ** self.time)\n",
    "\n",
    "            param.data -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Resets the parameters given to the optimizer\"\"\"\n",
    "        for param in self.params:\n",
    "            param.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f2f60",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "0dd17fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SGD\n",
      "step  0: w = [1.], loss = 25.0\n",
      "step  1: w = [1.8], loss = 16.0\n",
      "step  2: w = [2.44], loss = 10.240000000000002\n",
      "step  3: w = [2.952], loss = 6.553599999999998\n",
      "step  4: w = [3.362], loss = 4.194303999999998\n",
      "step  5: w = [3.689], loss = 2.6843545599999996\n",
      "step  6: w = [3.951], loss = 1.7179869183999996\n",
      "step  7: w = [4.161], loss = 1.0995116277759995\n",
      "step  8: w = [4.329], loss = 0.7036874417766399\n",
      "step  9: w = [4.463], loss = 0.4503599627370493\n",
      "step 10: w = [4.571], loss = 0.2882303761517112\n",
      "step 11: w = [4.656], loss = 0.184467440737095\n",
      "step 12: w = [4.725], loss = 0.11805916207174093\n",
      "step 13: w = [4.78], loss = 0.07555786372591429\n",
      "step 14: w = [4.824], loss = 0.04835703278458515\n",
      "step 15: w = [4.859], loss = 0.030948500982134555\n",
      "step 16: w = [4.887], loss = 0.019807040628566166\n",
      "step 17: w = [4.91], loss = 0.012676506002282305\n",
      "step 18: w = [4.928], loss = 0.008112963841460612\n",
      "step 19: w = [4.942], loss = 0.005192296858534741\n",
      "\n",
      "\n",
      "==\n",
      "\n",
      "\n",
      "Testing SGDMomentum\n",
      "step  0: w = [1.], loss = 25.0\n",
      "step  1: w = [2.7], loss = 16.0\n",
      "step  2: w = [4.69], loss = 5.289999999999999\n",
      "step  3: w = [6.543], loss = 0.09609999999999976\n",
      "step  4: w = [7.902], loss = 2.3808490000000004\n",
      "step  5: w = [8.545], loss = 8.42218441\n",
      "step  6: w = [8.414], loss = 12.566103316899996\n",
      "step  7: w = [7.614], loss = 11.658052243320999\n",
      "step  8: w = [6.371], loss = 6.83340535853089\n",
      "step  9: w = [4.978], loss = 1.8795944137086595\n",
      "step 10: w = [3.729], loss = 0.00048397153241860896\n",
      "step 11: w = [2.859], loss = 1.616162014766322\n",
      "step 12: w = [2.504], loss = 4.585519943964031\n",
      "step 13: w = [2.684], loss = 6.230991335289075\n",
      "step 14: w = [3.309], loss = 5.365188646350325\n",
      "step 15: w = [4.21], loss = 2.859863963884195\n",
      "step 16: w = [5.179], loss = 0.6244691059418507\n",
      "step 17: w = [6.015], loss = 0.03189968469707549\n",
      "step 18: w = [6.564], loss = 1.0298970334769058\n",
      "step 19: w = [6.746], loss = 2.447600739558188\n",
      "\n",
      "\n",
      "==\n",
      "\n",
      "\n",
      "Testing Adam\n",
      "step  0: w = [0.1], loss = 25.0\n",
      "step  1: w = [0.2], loss = 24.010000000980003\n",
      "step  2: w = [0.3], loss = 23.040554469276877\n",
      "step  3: w = [0.399], loss = 22.092004984027067\n",
      "step  4: w = [0.499], loss = 21.16467085819302\n",
      "step  5: w = [0.598], loss = 20.25884749655479\n",
      "step  6: w = [0.697], loss = 19.37480484125961\n",
      "step  7: w = [0.796], loss = 18.512785917898988\n",
      "step  8: w = [0.894], loss = 17.67300549416007\n",
      "step  9: w = [0.992], loss = 16.855648862027362\n",
      "step 10: w = [1.09], loss = 16.06087075331456\n",
      "step 11: w = [1.187], loss = 15.28879439700818\n",
      "step 12: w = [1.283], loss = 14.53951072553096\n",
      "step 13: w = [1.379], loss = 13.813077735607415\n",
      "step 14: w = [1.475], loss = 13.109520007963875\n",
      "step 15: w = [1.569], loss = 12.428828388642877\n",
      "step 16: w = [1.663], loss = 11.77095983328066\n",
      "step 17: w = [1.756], loss = 11.135837414306138\n",
      "step 18: w = [1.848], loss = 10.523350489687598\n",
      "step 19: w = [1.94], loss = 9.933355030594472\n"
     ]
    }
   ],
   "source": [
    "def test_optimizer(optimizer_cls):\n",
    "    print(\"Testing\", optimizer_cls.__name__)\n",
    "    w = Tensor([0.0], requires_grad=True)\n",
    "    optimizer = optimizer_cls([w], lr=0.1)\n",
    "\n",
    "    for step in range(20):\n",
    "        # Forward: simple quadratic loss\n",
    "        loss = ((w - Tensor([5.0])) ** 2).sum()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"step {step:2d}: w = {w.data}, loss = {loss.data}\")\n",
    "\n",
    "\n",
    "test_optimizer(SGD)\n",
    "print(\"\\n\\n==\\n\\n\")\n",
    "test_optimizer(SGDMomentum)\n",
    "print(\"\\n\\n==\\n\\n\")\n",
    "test_optimizer(Adam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd2f4b",
   "metadata": {},
   "source": [
    "## MLP Library\n",
    "\n",
    "Now that we have the `Tensor` class and optimizers, let's create the modules that will allow us to build a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "10c7fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \"\"\"Base class to construct neural networks\"\"\"\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Reset the gradients in the entire network\"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.grad = None\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"Construct a linear or fully-connected layer\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.W = Tensor(np.random.randn(out_dim, in_dim) * 0.01, requires_grad=True)\n",
    "        self.b = Tensor(np.zeros(out_dim), requires_grad=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not isinstance(x, Tensor):\n",
    "            x = Tensor(x)\n",
    "\n",
    "        # If single sample, add a batch dimension\n",
    "        added_batch = (x.data.ndim == 1)\n",
    "        if added_batch:\n",
    "            x = x.reshape(1, -1)\n",
    "\n",
    "        # (N, in_dim) @ (in_dim, out_dim) + (outdim,) -> (N, out_dim)\n",
    "        out = x @ self.W.transpose() + self.b\n",
    "\n",
    "        # If original input was 1D, squeeze back to (out_dim,)\n",
    "        if added_batch:\n",
    "            # Prefer reshape over indexing to avoid scatter-style backward\n",
    "            out = out.reshape(-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __call__(self, x):\n",
    "        return x.relu()\n",
    "\n",
    "\n",
    "class Tanh(Module):\n",
    "    def __call__(self, x):\n",
    "        return x.tanh()\n",
    "\n",
    "\n",
    "class Sigmoid(Module):\n",
    "    def __call__(self, x):\n",
    "        return x.sigmoid()\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    \"\"\"Construct a neural network of fully-connected layers with activation functions or\n",
    "       non-linearities (ReLU, Tanh, Sigmoid)\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, hidden, out_dim, activation=ReLU):\n",
    "        \"\"\"\n",
    "        Constructor for neural network\n",
    "\n",
    "        Params:\n",
    "            in_dim: int, number of dimensions for the input layer\n",
    "            hidden: list of int, list of dimensions for the hidden layers\n",
    "            out_dim: int, number of dimensions for the output layer\n",
    "            activation: callable, activation function to add to the linear layers\n",
    "        \"\"\"\n",
    "        dims = [in_dim] + hidden + [out_dim]\n",
    "\n",
    "        self.layers = []\n",
    "        for idx in range(len(dims) - 1):\n",
    "            # Add linear layer\n",
    "            self.layers.append(Linear(dims[idx], dims[idx + 1]))\n",
    "            # Add activation after every linear except the last one\n",
    "            if idx < len(dims) - 2:\n",
    "                self.layers.append(activation())\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [param for layer in self.layers for param in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6614b3c4",
   "metadata": {},
   "source": [
    "## Loss and Training Functions\n",
    "\n",
    "Let's add loss functions (mean-squared error for regression, cross-entropy for classification) and a function to perform the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "4eca8484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(pred, target):\n",
    "    \"\"\"Loss function that computes the mean-squared error\"\"\"\n",
    "    return ((pred - target) ** 2).mean()\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, target_idx):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss between raw logits and class index targets\n",
    "\n",
    "    Params:\n",
    "        logits: Tensor, the shape is (num_classes,) or (batch, num_classes)\n",
    "        target_idx: int, the correct class index/indices\n",
    "\n",
    "    Returns:\n",
    "        loss: Tensor, scalar cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Shift logits for numerical stability\n",
    "    logits_stable = logits - logits.max(axis=-1, keepdims=True)\n",
    "\n",
    "    # Convert logits to log probabilities with softmax\n",
    "    log_probs = logits_stable - logits_stable.exp().sum(axis=-1, keepdims=True).log()\n",
    "\n",
    "    # Single sample\n",
    "    if np.ndim(target_idx) == 0:\n",
    "        return -log_probs[target_idx]\n",
    "    # Mini-batch\n",
    "    else:\n",
    "        idx = np.arange(len(target_idx))\n",
    "        return (-log_probs[idx, target_idx]).mean()\n",
    "\n",
    "\n",
    "def train(model, data, labels, optimizer, loss_fn, epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train a model using the given optimizer and loss function\n",
    "\n",
    "    Params:\n",
    "        model: Module, the model to train (must implement __call__ and parameters())\n",
    "        data: iterable, input samples (list or array of Tensor or np.ndarray)\n",
    "        labels: iterable, target outputs (list or array of Tensor or np.ndarray)\n",
    "        optimizer: object, optimizer with step() and zero_grad() methods\n",
    "        loss_fn: function, computes a scalar loss given (pred, target)\n",
    "        epochs: int, number of passes over the dataset\n",
    "        batch_size: int, number of samples per batch\n",
    "    \"\"\"\n",
    "    num_samples = len(data)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Iterate in mini-batches\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = start + batch_size\n",
    "            x_batch = data[start:end].reshape(-1, 784)\n",
    "            y_batch = labels[start:end]\n",
    "\n",
    "            # Forward pass (model should handle batch input)\n",
    "            preds = model(x_batch)\n",
    "\n",
    "            # Compute loss (reduces to scalar inside loss_fn)\n",
    "            loss = loss_fn(preds, y_batch)\n",
    "\n",
    "            # Backward and optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += float(loss.data) * len(y_batch)\n",
    "\n",
    "        avg_loss = epoch_loss / num_samples\n",
    "        print(f\"epoch {epoch}: loss {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1595f3",
   "metadata": {},
   "source": [
    "## Training on MNIST\n",
    "\n",
    "Let's see if we can construct a neural network and train it on MNIST to validate the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "7aa46e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mnist\")\n",
    "\n",
    "dataset_train = dataset[\"train\"]\n",
    "dataset_test = dataset[\"test\"]\n",
    "\n",
    "X_train = np.array([np.array(img).reshape(-1) / 255.0 for img in dataset_train[\"image\"]])\n",
    "y_train = np.array(dataset_train[\"label\"])\n",
    "\n",
    "X_test = np.array([np.array(img).reshape(-1) / 255.0 for img in dataset_test[\"image\"]])\n",
    "y_test = np.array(dataset_test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "29b5b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss 0.7274\n",
      "epoch 1: loss 0.2214\n",
      "epoch 2: loss 0.1289\n",
      "epoch 3: loss 0.0890\n",
      "epoch 4: loss 0.0639\n",
      "epoch 5: loss 0.0497\n",
      "epoch 6: loss 0.0386\n",
      "epoch 7: loss 0.0312\n",
      "epoch 8: loss 0.0268\n",
      "epoch 9: loss 0.0233\n",
      "epoch 10: loss 0.0215\n",
      "epoch 11: loss 0.0195\n",
      "epoch 12: loss 0.0154\n",
      "epoch 13: loss 0.0151\n",
      "epoch 14: loss 0.0165\n",
      "epoch 15: loss 0.0144\n",
      "epoch 16: loss 0.0117\n",
      "epoch 17: loss 0.0108\n",
      "epoch 18: loss 0.0157\n",
      "epoch 19: loss 0.0099\n",
      "epoch 20: loss 0.0109\n",
      "epoch 21: loss 0.0118\n",
      "epoch 22: loss 0.0079\n",
      "epoch 23: loss 0.0107\n",
      "epoch 24: loss 0.0085\n",
      "epoch 25: loss 0.0094\n",
      "epoch 26: loss 0.0086\n",
      "epoch 27: loss 0.0086\n",
      "epoch 28: loss 0.0077\n",
      "epoch 29: loss 0.0057\n",
      "epoch 30: loss 0.0113\n",
      "epoch 31: loss 0.0079\n",
      "epoch 32: loss 0.0050\n",
      "epoch 33: loss 0.0092\n",
      "epoch 34: loss 0.0057\n",
      "epoch 35: loss 0.0051\n",
      "epoch 36: loss 0.0071\n",
      "epoch 37: loss 0.0076\n",
      "epoch 38: loss 0.0065\n",
      "epoch 39: loss 0.0070\n",
      "epoch 40: loss 0.0040\n",
      "epoch 41: loss 0.0058\n",
      "epoch 42: loss 0.0062\n",
      "epoch 43: loss 0.0058\n",
      "epoch 44: loss 0.0049\n",
      "epoch 45: loss 0.0072\n",
      "epoch 46: loss 0.0060\n",
      "epoch 47: loss 0.0035\n",
      "epoch 48: loss 0.0062\n",
      "epoch 49: loss 0.0043\n",
      "epoch 50: loss 0.0066\n",
      "epoch 51: loss 0.0054\n",
      "epoch 52: loss 0.0010\n",
      "epoch 53: loss 0.0068\n",
      "epoch 54: loss 0.0052\n",
      "epoch 55: loss 0.0076\n",
      "epoch 56: loss 0.0061\n",
      "epoch 57: loss 0.0022\n",
      "epoch 58: loss 0.0055\n",
      "epoch 59: loss 0.0043\n",
      "epoch 60: loss 0.0067\n",
      "epoch 61: loss 0.0038\n",
      "epoch 62: loss 0.0047\n",
      "epoch 63: loss 0.0048\n",
      "epoch 64: loss 0.0032\n",
      "epoch 65: loss 0.0041\n",
      "epoch 66: loss 0.0028\n",
      "epoch 67: loss 0.0043\n",
      "epoch 68: loss 0.0038\n",
      "epoch 69: loss 0.0051\n",
      "epoch 70: loss 0.0038\n",
      "epoch 71: loss 0.0026\n",
      "epoch 72: loss 0.0046\n",
      "epoch 73: loss 0.0034\n",
      "epoch 74: loss 0.0039\n",
      "epoch 75: loss 0.0028\n",
      "epoch 76: loss 0.0039\n",
      "epoch 77: loss 0.0042\n",
      "epoch 78: loss 0.0039\n",
      "epoch 79: loss 0.0036\n",
      "epoch 80: loss 0.0036\n",
      "epoch 81: loss 0.0038\n",
      "epoch 82: loss 0.0044\n",
      "epoch 83: loss 0.0017\n",
      "epoch 84: loss 0.0060\n",
      "epoch 85: loss 0.0035\n",
      "epoch 86: loss 0.0017\n",
      "epoch 87: loss 0.0033\n",
      "epoch 88: loss 0.0035\n",
      "epoch 89: loss 0.0042\n",
      "epoch 90: loss 0.0055\n",
      "epoch 91: loss 0.0036\n",
      "epoch 92: loss 0.0042\n",
      "epoch 93: loss 0.0018\n",
      "epoch 94: loss 0.0037\n",
      "epoch 95: loss 0.0053\n",
      "epoch 96: loss 0.0012\n",
      "epoch 97: loss 0.0022\n",
      "epoch 98: loss 0.0048\n",
      "epoch 99: loss 0.0027\n"
     ]
    }
   ],
   "source": [
    "neural_network = MLP(784, [256, 128, 10], 10)\n",
    "optimizer = Adam(params=neural_network.parameters())\n",
    "train(neural_network, X_train, y_train, optimizer, cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "d85231da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2806237125266356\n",
      "test accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "preds = neural_network(X_test)\n",
    "loss = cross_entropy_loss(preds, y_test)\n",
    "print(\"test loss:\", float(loss.data))\n",
    "\n",
    "pred_labels = preds.data.argmax(axis=1)\n",
    "accuracy = (pred_labels == y_test).mean()\n",
    "print(\"test accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
