{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0386700b",
   "metadata": {},
   "source": [
    "# Minimal Transformer (PyTorch)\n",
    "\n",
    "This notebook contains a minimal implementation of the Transformer architecture using PyTorch's autograd engine. It covers the core building blocks of attention-based models:\n",
    "- Scaled Dot-Product Attention: compute attention weights from queries, keys, and values\n",
    "- Multi-Head Attention: project into multiple subspaces and combine parallel attention heads\n",
    "- Residual Connections and LayerNorm: stabilize training with skip connections and normalization\n",
    "- Feed-Forward Network with GELU: apply non-linear transformations between attention layers\n",
    "- Encoder and Decoder Blocks: stack layers to form the Transformer backbone"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
