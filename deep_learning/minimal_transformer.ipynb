{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0386700b",
   "metadata": {},
   "source": [
    "# Minimal Transformer (PyTorch)\n",
    "\n",
    "This notebook contains a minimal implementation of the Transformer architecture using PyTorch's autograd engine. It covers the core building blocks of attention-based models:\n",
    "- Scaled Dot-Product Attention: compute attention weights from queries, keys, and values\n",
    "- Multi-Head Attention: project into multiple subspaces and combine parallel attention heads\n",
    "- Residual Connections and LayerNorm: stabilize training with skip connections and normalization\n",
    "- Feed-Forward Network with GELU: apply non-linear transformations between attention layers\n",
    "- Encoder and Decoder Blocks: stack layers to form the Transformer backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c618e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117c77ab0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "np.set_printoptions(precision=3)\n",
    "np.random.seed(0)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6100c",
   "metadata": {},
   "source": [
    "## Tokens and Embeddings\n",
    "\n",
    "Before attention, raw text is turned into vectors through **tokenization** and **embedding**. Tokenization splits a sentence into smaller units. In practice, this is usually **subword pieces** (e.g. \"Transformers\" $\\rightarrow$ [\"Trans\", \"former\", \"s\"]), each mapped to an integer ID from the vocabulary.\n",
    "\n",
    "Each ID is then mapped to a **word embedding**, a vector of size $d_{model}$ (often 128â€“1024+). A **positional encoding** is often added to represent where each token appears in the sequence. After this step, a sequence of length $n$ is represented as a matrix of shape $(n, d_{model})$ where each row is a token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19cfcaf",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "The goal of attention is to decide **how much each token should look at other tokens** when forming a new representation.\n",
    "\n",
    "From the input matrix, the model forms three projections, queries $Q$, keys $K$, and values $V$. Each is obtained by multiplying the input matrix by a different weight matrix ($W_Q$, $W_K$, $W_V$). Intuitively, each token produces a query that asks what it wants, a key that signals what it offers, and a value that carries the content to be shared. These are the inputs to the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62a82e",
   "metadata": {},
   "source": [
    "### Mechanism\n",
    "\n",
    "Attention works by comparing queries and keys to decide how much weight to give each value. First, we compute similarity scores between every query and key.\n",
    "\n",
    "$$\n",
    "scores = Q K^T\n",
    "$$\n",
    "\n",
    "To prevent these values from growing too large as the dimension $d_k$ increases, the scores tend to be scaled.\n",
    "\n",
    "$$\n",
    "scores_{scaled} = \\frac{Q K^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "The scaled scores are then passed through a softmax, which normalizes each row into a probability distribution (so the sum of all entries is $1$).\n",
    "\n",
    "$$\n",
    "\\alpha = \\text{softmax}(scores_{scaled})\n",
    "$$\n",
    "\n",
    "Finally, the output of attention is obtained by weighting the values by these probabilities:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\alpha V\n",
    "$$\n",
    "\n",
    "In words, each token's new representation is a weighted combination of the value vectors of all tokens in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0349f4",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "Sometimes we want to restrict which tokens can attend to which others. This is done with a **mask** applied to the attention scores before the softmax.\n",
    "\n",
    "- **Padding mask:** prevents the model from attending to padded positions in sequences of different lengths.\n",
    "- **Causal mask:** prevents a token from looking ahead at future tokens, which is required in the decoder for autoregressive generation.\n",
    "\n",
    "In practice, masked positions are set to $-\\infty$ so that their softmax probability becomes zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5069a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute attention(Q, K, V) = softmax(Q K^T / sqrt(d_k)) * V with optional masking\n",
    "\n",
    "    Args:\n",
    "        Q: (num_batches, sequence_length, d_k)\n",
    "        K: (num_batches, sequence_length, d_k)\n",
    "        V: (num_batches, sequence_length, d_v)\n",
    "\n",
    "    Returns:\n",
    "        H: (num_batches, sequence_length, d_v)\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalizing factor for stability\n",
    "    d_k = Q.size(-1)\n",
    "\n",
    "    # Compute similarity scores between queries and keys\n",
    "    scores = Q @ K.transpose(-2, -1) / d_k**0.5\n",
    "\n",
    "    # Apply optional mask\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Convert to probability distribution\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Get weighted average of value vectors based on similarity scores of queries and keys\n",
    "    return weights @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553aaeb",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Scaled dot-product attention works with a single set of queries, keys, and values. Multi-head attention extends this idea by applying attention multiple times in parallel, with each head having its own learned projections. The purpose is to let the model capture different types of relationships: one head might focus on nearby words, another on long-range dependencies, and others may discover more abstract patterns that go beyond grammatical structures.\n",
    "\n",
    "Formally, for head $i$:\n",
    "$$\n",
    "head_i = \\text{Attention}(X W_Q^{(i)}, X W_K^{(i)}, X W_V^{(i)})\n",
    "$$\n",
    "where $X \\in \\mathbb{R}^{n \\times d_{model}}$ is the input sequence, and $W_Q^{(i)}, W_K^{(i)}, W_V^{(i)}$ are learned weight matrices specific to head $i$.\n",
    "\n",
    "After computing all $h$ heads, the results are concatenated and projected back into the original model dimension:\n",
    "$$\n",
    "\\text{MultiHead}(X) = \\text{Concat}(head_1, \\dots, head_h) \\cdot W_O\n",
    "$$\n",
    "where $W_O$ is another learned weight matrix.\n",
    "\n",
    "In terms of shapes, the input has shape $(n, d_{model})$. It is projected and reshaped into $(num\\_heads, n, d_k)$ so that each head works in a subspace of dimension $d_k = d_{model} / num\\_heads$. Each head runs scaled dot-product attention independently. The outputs are then concatenated back into $(n, d_{model})$ and passed through $W_O$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1463d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        # Check that token embedding size is divisible by number of heads\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # d_model = num_heads * d_k so these are the weights of each head stacked together\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        B, n, _ = X.size()\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_Q(X).view(B, n, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_K(X).view(B, n, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_V(X).view(B, n, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Run attention per head\n",
    "        H = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine heads\n",
    "        H = H.transpose(1, 2).contiguous().view(B, n, -1)\n",
    "        return self.W_O(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b07a5",
   "metadata": {},
   "source": [
    "## Feedforward Block (FFN, GELU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961fc6c",
   "metadata": {},
   "source": [
    "## Residual Connections and Layer Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634ac1e",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d043da30",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ce0d6",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b68c21",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
