{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0386700b",
   "metadata": {},
   "source": [
    "# Minimal Transformer (PyTorch)\n",
    "\n",
    "This notebook contains a minimal implementation of the Transformer architecture using PyTorch's autograd engine. It covers the core building blocks of attention-based models:\n",
    "- Scaled Dot-Product Attention: compute attention weights from queries, keys, and values\n",
    "- Multi-Head Attention: project into multiple subspaces and combine parallel attention heads\n",
    "- Residuals and LayerNorm: stabilize training with skip connections and normalization\n",
    "- Feed-Forward Network with GELU: apply non-linear transformations between attention layers\n",
    "- Encoder and Decoder Blocks: stack layers to form the Transformer backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403da082",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c618e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117c77ab0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "np.set_printoptions(precision=3)\n",
    "np.random.seed(0)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6100c",
   "metadata": {},
   "source": [
    "## Tokens and Embeddings\n",
    "\n",
    "Before attention, raw text is turned into vectors through **tokenization** and **embedding**. Tokenization splits a sentence into smaller units. In practice, this is usually **subword pieces** (e.g. \"Transformers\" $\\rightarrow$ [\"Trans\", \"former\", \"s\"]), each mapped to an integer ID from the vocabulary.\n",
    "\n",
    "Each ID is then mapped to a **word embedding**, a vector of size $d_{model}$ (often 128-1024+). A **positional encoding** is often added to represent where each token appears in the sequence. After this step, a sequence of length $n$ is represented as a matrix of shape $(n, d_{model})$ where each row is a token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19cfcaf",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "The goal of attention is to decide **how much each token should look at other tokens** when forming a new representation.\n",
    "\n",
    "From the input matrix, the model forms three projections, queries $Q$, keys $K$, and values $V$. Each is obtained by multiplying the input matrix by a different weight matrix ($W_Q$, $W_K$, $W_V$). Intuitively, each token produces a query that asks what it wants, a key that signals what it offers, and a value that carries the content to be shared. These are the inputs to the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62a82e",
   "metadata": {},
   "source": [
    "### Mechanism\n",
    "\n",
    "Attention works by comparing queries and keys to decide how much weight to give each value. First, we compute similarity scores between every query and key.\n",
    "\n",
    "$$\n",
    "scores = Q K^T\n",
    "$$\n",
    "\n",
    "To prevent these values from growing too large as the dimension $d_k$ increases, the scores tend to be scaled.\n",
    "\n",
    "$$\n",
    "scores_{scaled} = \\frac{Q K^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "The scaled scores are then passed through a softmax, which normalizes each row into a probability distribution (so the sum of all entries is $1$).\n",
    "\n",
    "$$\n",
    "\\alpha = \\text{softmax}(scores_{scaled})\n",
    "$$\n",
    "\n",
    "Finally, the output of attention is obtained by weighting the values by these probabilities:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\alpha V\n",
    "$$\n",
    "\n",
    "In words, each token's new representation is a weighted combination of the value vectors of all tokens in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0349f4",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "Sometimes we want to restrict which tokens can attend to which others. This is done with a **mask** applied to the attention scores before the softmax.\n",
    "\n",
    "- **Padding mask:** prevents the model from attending to padded positions in sequences of different lengths.\n",
    "- **Causal mask:** prevents a token from looking ahead at future tokens, which is required in the decoder for autoregressive generation.\n",
    "\n",
    "In practice, masked positions are set to $-\\infty$ so that their softmax probability becomes zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5069a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute attention(Q, K, V) = softmax(Q K^T / sqrt(d_k)) * V with optional masking\n",
    "\n",
    "    Args:\n",
    "        Q: (num_batches, sequence_length, d_k)\n",
    "        K: (num_batches, sequence_length, d_k)\n",
    "        V: (num_batches, sequence_length, d_v)\n",
    "\n",
    "    Returns:\n",
    "        H: (num_batches, sequence_length, d_v)\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalizing factor for stability\n",
    "    d_k = Q.size(-1)\n",
    "\n",
    "    # Compute similarity scores between queries and keys\n",
    "    scores = Q @ K.transpose(-2, -1) / d_k**0.5\n",
    "\n",
    "    # Apply optional mask\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Convert to probability distribution\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Get weighted average of value vectors based on similarity scores of queries and keys\n",
    "    return weights @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553aaeb",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Scaled dot-product attention works with a single set of queries, keys, and values. Multi-head attention extends this idea by applying attention multiple times in parallel, with each head having its own learned projections. The purpose is to let the model capture different types of relationships: one head might focus on nearby words, another on long-range dependencies, and others may discover more abstract patterns that go beyond grammatical structures.\n",
    "\n",
    "For head $i$:\n",
    "$$\n",
    "head_i = \\text{Attention}(X W_Q^{(i)}, X W_K^{(i)}, X W_V^{(i)})\n",
    "$$\n",
    "where $X \\in \\mathbb{R}^{n \\times d_{model}}$ is the input sequence, and $W_Q^{(i)}, W_K^{(i)}, W_V^{(i)}$ are learned weight matrices specific to head $i$.\n",
    "\n",
    "After computing all $h$ heads, the results are concatenated and projected back into the original model dimension:\n",
    "$$\n",
    "\\text{MultiHead}(X) = \\text{Concat}(head_1, \\dots, head_h) \\cdot W_O\n",
    "$$\n",
    "where $W_O$ is another learned weight matrix.\n",
    "\n",
    "In terms of shapes, the input has shape $(n, d_{model})$. It is projected and reshaped into $(num\\_heads, n, d_k)$ so that each head works in a subspace of dimension $d_k = d_{model} / num\\_heads$. Each head runs scaled dot-product attention independently. The outputs are then concatenated back into $(n, d_{model})$ and passed through $W_O$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1463d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        # Check that token embedding size is divisible by number of heads\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # d_model = num_heads * d_k so these are the weights of each head stacked together\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        B, n, _ = X.size()\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_Q(X).view(B, n, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_K(X).view(B, n, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_V(X).view(B, n, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Run attention per head\n",
    "        H = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine heads\n",
    "        H = H.transpose(1, 2).contiguous().view(B, n, -1)\n",
    "        return self.W_O(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b07a5",
   "metadata": {},
   "source": [
    "## Feed-Forward Network (FFN) with GELU\n",
    "\n",
    "After multi-head attention, each position in the sequence is processed independently by a feed-forward network. This network applies the same transformation to every token embedding, without mixing information across sequence positions. Its purpose is to increase the expressive power of the model by introducing non-linearity and enabling richer transformations.\n",
    "\n",
    "For an input $x \\in \\mathbb{R}^{d_{model}}$:\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{GELU}(x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $W_1 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$ and $b_1 \\in \\mathbb{R}^{d_{ff}}$\n",
    "- $W_2 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$ and $b_2 \\in \\mathbb{R}^{d_{model}}$\n",
    "- $d_{ff}$ is typically larger than $d_{model}$ (e.g. 2048 vs. 512), creating an “expansion” layer before projecting back down.\n",
    "\n",
    "The non-linearity used here is the **Gaussian Error Linear Unit (GELU)**, which smoothly scales negative values instead of zeroing them out (as ReLU would).\n",
    "\n",
    "In terms of shapes:\n",
    "- The input sequence has shape $(n, d_{model})$.\n",
    "- It is projected into $(n, d_{ff})$, passed through GELU, then projected back into $(n, d_{model})$.\n",
    "- Each token is transformed independently, but using the same learned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5019f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "\n",
    "        # First projection expands the hidden dimension\n",
    "        self.W1 = nn.Linear(d_model, d_ff)\n",
    "\n",
    "        # Second projection projects back to model dimension\n",
    "        self.W2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        # GELU activation\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: (batch_size, sequence_length, d_model)\n",
    "\n",
    "        Returns:\n",
    "            Output: (batch_size, sequence_length, d_model)\n",
    "        \"\"\"\n",
    "        return self.W2(self.activation(self.W1(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961fc6c",
   "metadata": {},
   "source": [
    "## Residual Connections\n",
    "\n",
    "Imagine each Transformer sublayer (like attention or the feed-forward network) as a function that transforms its input. Without a residual connection, the model would have to **completely rewrite the representation** at every layer, which can make training unstable and slow.\n",
    "\n",
    "Residual connections fix this by **adding the original input back in** after the sublayer transformation.\n",
    "This means:\n",
    "- The sublayer only needs to **learn a correction or refinement** to the input, not a full replacement.\n",
    "- During backpropagation, gradients can flow directly through the skip path, which reduces the risk of vanishing gradients in very deep models.\n",
    "\n",
    "For a sublayer function $\\text{Sublayer}(x)$:\n",
    "$$\n",
    "\\text{Residual}(x) = x + \\text{Sublayer}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b72ea",
   "metadata": {},
   "source": [
    "### Example: Residual Connections Illustration\n",
    "\n",
    "Suppose the input vector is $x = [2, 3]$, and the sublayer produces $\\text{Sublayer}(x) = [5, -1]$. Without a residual, the output would simply be $[5, -1]$, completely replacing the input. With a residual (skip connection), we add the input back:\n",
    "$$\n",
    "\\text{Output} = x + \\text{Sublayer}(x) = [2, 3] + [5, -1] = [7, 2]\n",
    "$$\n",
    "\n",
    "Now the sublayer only needs to learn a refinement of $x$ rather than rebuild it from scratch. If the sublayer output were $[0, 0]$, the residual path would still pass $x$ forward unchanged. This means residuals let the network preserve useful information, make each layer act as a tweak instead of a rewrite, and provide a direct path for gradients, which stabilizes training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8faac82",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "After adding the residual, a **Layer Normalization** step is applied. LayerNorm normalizes activations across the feature dimension for each token, stabilizing training and ensuring consistent scale.\n",
    "\n",
    "For input $x \\in \\mathbb{R}^{d_{model}}$:\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mu$ and $\\sigma$ are the mean and standard deviation of the features in $x$  \n",
    "- $\\gamma, \\beta \\in \\mathbb{R}^{d_{model}}$ are learnable parameters that scale and shift the normalized values  \n",
    "\n",
    "Each sublayer in the Transformer (multi-head attention, feed-forward) is wrapped as:\n",
    "$$\n",
    "\\text{Output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79fa283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Apply residual connection to any sublayer with the same input/output shape.\n",
    "\n",
    "        Args:\n",
    "            x: (batch_size, sequence_length, d_model)\n",
    "            sublayer: a function or nn.Module that takes x and returns (batch_size, sequence_length, d_model)\n",
    "\n",
    "        Returns:\n",
    "            Output: (batch_size, sequence_length, d_model)\n",
    "        \"\"\"\n",
    "        return self.norm(x + self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634ac1e",
   "metadata": {},
   "source": [
    "## Encoder Block\n",
    "\n",
    "A Transformer encoder block stacks the core components including multi-head attention, a feed-forward network, and residual connections with layer normalization. Each block processes the input sequence:\n",
    "- Apply multi-head attention with residual and layer normalization\n",
    "- Apply a position-wise feed-forward network with residual and layer normalization\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{EncoderBlock}(x) \n",
    "&= \\text{LayerNorm}(x + \\text{MultiHeadAttention}(x)) \\\\\n",
    "&\\to \\text{LayerNorm}(x + \\text{FeedForward}(x))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This structure allows the model to mix information across sequence positions (via attention) and then transform each position independently (via the FFN), while residuals and normalization ensure stable training and consistent dimensions throughout the stack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26c89840",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.residual1 = ResidualConnection(d_model, dropout)\n",
    "        self.residual2 = ResidualConnection(d_model, dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, sequence_length, d_model)\n",
    "            mask: optional attention mask\n",
    "\n",
    "        Returns:\n",
    "            Output: (batch_size, sequence_length, d_model)\n",
    "        \"\"\"\n",
    "        # Multi-head attention with residual + norm\n",
    "        x = self.residual1(x, lambda x: self.attention(x, mask))\n",
    "\n",
    "        # Feed-forward with residual + norm\n",
    "        x = self.residual2(x, self.ffn)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d043da30",
   "metadata": {},
   "source": [
    "## Decoder Block\n",
    "\n",
    "A Transformer decoder block extends the encoder structure with two attention layers:\n",
    "- Apply **masked multi-head self-attention** with residual and layer normalization (causal mask prevents each position from attending to future tokens)\n",
    "- Apply **encoder-decoder cross-attention** with residual and layer normalization (queries come from the decoder, keys and values from the encoder output)\n",
    "- Apply a position-wise feed-forward network with residual and layer normalization\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{DecoderBlock}(x, \\text{enc\\_out})\n",
    "&= \\text{LayerNorm}(x + \\text{MaskedSelfAttention}(x)) \\\\\n",
    "&\\to \\text{LayerNorm}(x + \\text{CrossAttention}(x, \\text{enc\\_out})) \\\\\n",
    "&\\to \\text{LayerNorm}(x + \\text{FeedForward}(x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This structure allows the decoder to (1) build up its own representation autoregressively, (2) attend to the encoder's representation of the input sequence, and (3) refine token embeddings through the feed-forward network, all while residuals and normalization maintain stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7a6c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "\n",
    "        self.residual1 = ResidualConnection(d_model, dropout)\n",
    "        self.residual2 = ResidualConnection(d_model, dropout)\n",
    "        self.residual3 = ResidualConnection(d_model, dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, target_length, d_model) decoder input\n",
    "            enc_out: (batch_size, source_length, d_model) encoder output\n",
    "            src_mask: optional mask for encoder attention\n",
    "            tgt_mask: optional mask for decoder self-attention (causal mask)\n",
    "\n",
    "        Returns:\n",
    "            Output: (batch_size, target_length, d_model)\n",
    "        \"\"\"\n",
    "        # Masked self-attention (causal)\n",
    "        x = self.residual1(x, lambda x: self.self_attn(x, tgt_mask))\n",
    "\n",
    "        # Cross-attention (queries from decoder, keys/values from encoder)\n",
    "        x = self.residual2(x, lambda x: self.cross_attn(x, src_mask))\n",
    "\n",
    "        # Feed-forward network\n",
    "        x = self.residual3(x, self.ffn)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ce0d6",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "The Transformer consists of an **encoder stack** and a **decoder stack**:\n",
    "- The **encoder** is a stack of $N$ identical encoder blocks, each with multi-head self-attention and a feed-forward network, connected with residuals and layer normalization.\n",
    "- The **decoder** is a stack of $N$ decoder blocks, each with masked self-attention, encoder-decoder cross-attention, and a feed-forward network, also wrapped with residuals and layer normalization.\n",
    "\n",
    "The encoder produces hidden representations $H = \\text{Encoder}(X)$ for the input sequence $X$, and the decoder generates the output sequence autoregressively as:\n",
    "$$\n",
    "Y = \\text{Decoder}(Y_{<t}, H)\n",
    "$$\n",
    "\n",
    "This design enables the model to capture long-range dependencies, align source and target sequences through cross-attention, and generate outputs in parallel during encoding and sequentially during decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0cf858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, src_mask, tgt_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, src_vocab_size, tgt_vocab_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.src_embed = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch_size, src_len) - source token indices\n",
    "            tgt: (batch_size, tgt_len) - target token indices\n",
    "            src_mask: optional source mask\n",
    "            tgt_mask: optional target (causal) mask\n",
    "\n",
    "        Returns:\n",
    "            Output logits: (batch_size, tgt_len, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        # Embedding lookup\n",
    "        src = self.src_embed(src)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "\n",
    "        # Encode source\n",
    "        enc_out = self.encoder(src, src_mask)\n",
    "\n",
    "        # Decode target\n",
    "        dec_out = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
    "\n",
    "        # Project to vocab size\n",
    "        return self.output_layer(dec_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b68c21",
   "metadata": {},
   "source": [
    "## Training and Validation\n",
    "\n",
    "A simple way to validate the Transformer is with a toy copy task where the model is trained to reproduce its input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ed9694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss = 2.5311\n",
      "Step 20, Loss = 0.2345\n",
      "Step 40, Loss = 0.0186\n",
      "Step 60, Loss = 0.0071\n",
      "Step 80, Loss = 0.0039\n",
      "Step 100, Loss = 0.0025\n"
     ]
    }
   ],
   "source": [
    "# Tiny Toy Dataset: copy task\n",
    "src_vocab_size = tgt_vocab_size = 10\n",
    "seq_len, batch_size = 5, 2\n",
    "src = torch.randint(1, src_vocab_size, (batch_size, seq_len))\n",
    "# Target is same as input\n",
    "tgt = src.clone()\n",
    "\n",
    "# Shift target for decoder input\n",
    "dec_in = torch.zeros_like(tgt)\n",
    "dec_in[:, 1:] = tgt[:, :-1]\n",
    "\n",
    "# Small model\n",
    "model = Transformer(d_model=16, num_heads=2, d_ff=32,\n",
    "                    num_layers=1, src_vocab_size=src_vocab_size,\n",
    "                    tgt_vocab_size=tgt_vocab_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Simple causal mask\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Training loop\n",
    "for step in range(101):\n",
    "    out = model(src, dec_in, tgt_mask=mask)  # (batch, seq_len, vocab)\n",
    "    loss = criterion(out.view(-1, tgt_vocab_size), tgt.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 20 == 0:\n",
    "        print(f\"Step {step}, Loss = {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
