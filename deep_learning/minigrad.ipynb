{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328e4647",
   "metadata": {},
   "source": [
    "# Minigrad Autodiff Engine\n",
    "\n",
    "This notebook contains a minimal implementation of an automatic differentiation engine that supports reverse-mode autodiff on N-dimensional tensors:\n",
    "- Tensor: core data structure and computation graph\n",
    "- Broadcasting & reductions: broadcasting in forward pass and unbroadcasting in backward pass\n",
    "- Matmul: matrix multiplication and its vector-Jacobian products (VJPs)\n",
    "- NN module: neural network library with Linear layers, activations, parameter collection\n",
    "- Experiments: gradient checking, training simple models on toy datasets\n",
    "\n",
    "Note: this project is self-contained and won't be shared with CNN / Transformer / Diffusion, those projects will use Pytorch autograd.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc48969",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0451c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)\n",
    "np.random.seed(0)\n",
    "\n",
    "def _unbroadcast(grad, target_shape):\n",
    "    \"\"\"\n",
    "    Reduce a gradient to match the given target shape.\n",
    "\n",
    "    Params:\n",
    "        grad: np.array, the gradient with the broadcasted shape\n",
    "        target_shape: tuple of int, the shape of the tensor before broadcasting\n",
    "\n",
    "    Returns:\n",
    "        result: np.array, the gradient with reduced dimensions and summed so it matches the target shape\n",
    "    \"\"\"\n",
    "    # grad.ndim is number of axes, len(target_shape) is desired number of axes.\n",
    "    # For [[a, b], [c, d]], the result of sum(axis=0) is [a+c, b+d]\n",
    "    while grad.ndim > len(target_shape):\n",
    "        grad = grad.sum(axis=0)\n",
    "    \n",
    "    # For every axis where the original tensor had size 1 and was broadcasted, we sum\n",
    "    # the gradient along that axis to collapse it back.\n",
    "    for axis, dim in reversed(list(enumerate(target_shape))):\n",
    "        if dim == 1:\n",
    "            grad = grad.sum(axis=axis, keepdims=True)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfed5d6",
   "metadata": {},
   "source": [
    "## Tensor Data Structure\n",
    "\n",
    "The `Tensor` is a core data structure used to construct computation graphs or neural networks that process input data, generate predictions, and evaluate a loss function. It also enables us to perform backpropagation to compute gradients at each node so the network's weights can be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41ac9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=False):\n",
    "        # Numpy scalar, vector, matrix\n",
    "        self.data = np.array(data, dtype=np.float64)\n",
    "\n",
    "        # Determines if we should store gradient\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        # Gradient of the loss wrt this tensor (same shape as .data)\n",
    "        self.grad = None\n",
    "\n",
    "        # Closure that computes local gradients and accumulates them into parent tensors\n",
    "        self._backward = lambda: None\n",
    "\n",
    "        # Parent tensors\n",
    "        self._prev = ()\n",
    "\n",
    "        # Op name for logging\n",
    "        self._op = ''\n",
    "\n",
    "    def __repr__(self):\n",
    "        # String representation for logging\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad}, op={self._op})\"\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Performs backpropagation on the computation graph and updates internal variables.\"\"\"\n",
    "        if self.data.shape != ():\n",
    "            raise ValueError(\"backward() assumes scalar output on the loss layer\")\n",
    "\n",
    "        # Set loss gradient to ones\n",
    "        self.grad = np.array(1.0, dtype=np.float64)\n",
    "\n",
    "        # Build DFS topological sort of computation graph\n",
    "        topo, visited = [], set()\n",
    "        def build(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for p in v._prev: \n",
    "                    build(p)\n",
    "                topo.append(v)\n",
    "        build(self)\n",
    "\n",
    "        # Reverse topo sort order (starting from the outermost / loss layer)\n",
    "        for v in reversed(topo):\n",
    "            if v.grad is not None:\n",
    "                v._backward()\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"Elementwise addition with broadcasting, returns a new Tensor and sets up gradient flow to both parents\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "\n",
    "        # Construct new tensor resulting from the operation\n",
    "        output_data = self.data + other.data\n",
    "        output = Tensor(data=output_data, requires_grad=(self.requires_grad or other.requires_grad))\n",
    "        output._prev = (self, other)\n",
    "        output._op = \"add\"\n",
    "\n",
    "        # For the new tensor, set backprop method to update the gradients of its parents.\n",
    "        # For addition, the local derivative with respect to each parent is 1, so the upstream\n",
    "        # gradient is passed straight through.\n",
    "        # .grad has the gradient with respect to the outermost layer.\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad_self = _unbroadcast(output.grad, self.data.shape)\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "            if other.requires_grad:\n",
    "                grad_other = _unbroadcast(output.grad, other.data.shape)\n",
    "                other.grad = grad_other if other.grad is None else other.grad + grad_other\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Elementwise multiplication with broadcasting, returns a new Tensor and sets up gradient flow to both parents\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "\n",
    "        output_data = self.data * other.data\n",
    "        output = Tensor(data=output_data, requires_grad=(self.requires_grad or other.requires_grad))\n",
    "        output._prev = (self, other)\n",
    "        output._op = \"mul\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad_other = output.grad * other.data\n",
    "                grad_other = _unbroadcast(grad_other, self.data.shape)\n",
    "                self.grad = grad_other if self.grad is None else self.grad + grad_other\n",
    "            if other.requires_grad:\n",
    "                grad_self = output.grad * self.data\n",
    "                grad_self = _unbroadcast(grad_self, other.data.shape)\n",
    "                other.grad = grad_self if other.grad is None else other.grad + grad_self\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"Matrix multiplication (@ operator), returns a new Tensor and sets up gradient flow to both parents\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "\n",
    "        output_data = self.data @ other.data\n",
    "        output = Tensor(data=output_data, requires_grad=(self.requires_grad or other.requires_grad))\n",
    "        output._prev = (self, other)\n",
    "        output._op = \"matmul\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if other.data.ndim == 1:\n",
    "                    grad_self = np.outer(output.grad, other.data)\n",
    "                else:\n",
    "                    grad_self = output.grad @ other.data.T\n",
    "                grad_self = _unbroadcast(grad_self, self.data.shape)\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "            if other.requires_grad:\n",
    "                if self.data.ndim == 1:\n",
    "                    grad_other = np.outer(self.data, output.grad)\n",
    "                else:\n",
    "                    grad_other = self.data.T @ output.grad\n",
    "                grad_self = _unbroadcast(grad_self, other.data.shape)\n",
    "                other.grad = grad_other if other.grad is None else other.grad + grad_other\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negative operator, uses predefined ops\"\"\"\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        \"\"\"Negative operator, uses predefined ops\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        \"\"\"Reverse subtraction operator, uses predefined ops\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        return other + (-self)\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        \"\"\"Power (** operator), returns a new Tensor and sets up gradient flow to both parents\"\"\"\n",
    "        assert isinstance(power, (int, float)), \"only scalar powers supported\"\n",
    "        output_data = self.data ** power\n",
    "        output = Tensor(output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"pow\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad_self = output.grad * (power * (self.data ** (power - 1)))\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        \"\"\"True division, uses predefined ops\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        return self * (other ** -1)\n",
    "\n",
    "    def exp(self):\n",
    "        \"\"\"e^x, returns a new Tensor and sets up gradient flow to both parents\"\"\"\n",
    "        output_data = np.exp(self.data)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"exp\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad_self = output.grad * output.data\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def log(self):\n",
    "        \"\"\"Natural logarithm, returns a new Tensor and sets up gradient flow to the parent\"\"\"\n",
    "        output_data = np.log(self.data)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"log\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad_self = output.grad * (1.0 / self.data)\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def relu(self):\n",
    "        \"\"\"ReLU activation, returns a new Tensor and sets up gradient flow to the parent\"\"\"\n",
    "        output_data = np.maximum(0, self.data)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"relu\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad_self = output.grad * (self.data > 0).astype(self.data.dtype)\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def sigmoid(self):\n",
    "        \"\"\"Sigmoid activation, returns a new Tensor and sets up gradient flow to the parent\"\"\"\n",
    "        output_data = 1 / (1 + np.exp(-self.data))\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"sigmoid\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad_self = output.grad * (output.data * (1 - output.data))\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def tanh(self):\n",
    "        \"\"\"Tanh activation, returns a new Tensor and sets up gradient flow to the parent\"\"\"\n",
    "        output_data = np.tanh(self.data)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"tanh\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad_self = output.grad * (1 - output.data ** 2)\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def sum(self, axis=None, keepdims=False):\n",
    "        \"\"\"Sum of tensor elements over given axes, returns a new Tensor\"\"\"\n",
    "        output_data = self.data.sum(axis=axis, keepdims=keepdims)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"sum\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad_self = output.grad\n",
    "                if not keepdims and axis is not None:\n",
    "                    grad_self = np.expand_dims(grad_self, axis=axis)\n",
    "                grad_self = np.broadcast_to(grad_self, self.data.shape)\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def mean(self, axis=None, keepdims=False):\n",
    "        \"\"\"Mean of tensor elements over given axes, returns a new Tensor\"\"\"\n",
    "        output_data = self.data.mean(axis=axis, keepdims=keepdims)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"mean\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad_self = output.grad\n",
    "                if not keepdims and axis is not None:\n",
    "                    grad_self = np.expand_dims(grad_self, axis=axis)\n",
    "                if axis is None:\n",
    "                    denom = self.data.size\n",
    "                else:\n",
    "                    denom = self.data.shape[axis]\n",
    "                grad_self = np.broadcast_to(grad_self, self.data.shape) / denom\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def reshape(self, *shape):\n",
    "        \"\"\"Reshape tensor to given shape, returns a new Tensor\"\"\"\n",
    "        output_data = self.data.reshape(*shape)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"reshape\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad_self = output.grad.reshape(self.data.shape)\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def transpose(self, *axes):\n",
    "        \"\"\"Transpose tensor according to given axes, returns a new Tensor\"\"\"\n",
    "        output_data = self.data.transpose(*axes)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"transpose\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if axes:\n",
    "                    inv_axes = np.argsort(axes)\n",
    "                    grad_self = output.grad.transpose(*inv_axes)\n",
    "                else:\n",
    "                    grad_self = output.grad.transpose()\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def broadcast_to(self, shape):\n",
    "        \"\"\"Broadcast tensor to given shape, returns a new Tensor\"\"\"\n",
    "        output_data = np.broadcast_to(self.data, shape)\n",
    "        output = Tensor(data=output_data, requires_grad=self.requires_grad)\n",
    "        output._prev = (self,)\n",
    "        output._op = \"broadcast_to\"\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad_self = _unbroadcast(output.grad, self.data.shape)\n",
    "                self.grad = grad_self if self.grad is None else self.grad + grad_self\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133170c",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "We can try out a few basic models to see if the Tensor operations are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b378647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: Tensor(data=[1. 2.], grad=[0. 2.], op=)\n",
      "W: Tensor(data=[[1. 2.]\n",
      " [1. 1.]], grad=[[ 2.  4.]\n",
      " [-2. -4.]], op=)\n",
      "b: Tensor(data=[1. 1.], grad=[ 2. -2.], op=)\n",
      "y: Tensor(data=[6. 4.], grad=[ 2. -2.], op=add)\n",
      "label: Tensor(data=[5. 5.], grad=None, op=)\n",
      "loss: Tensor(data=2.0, grad=1.0, op=sum)\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([1, 2], requires_grad=True)\n",
    "W = Tensor([[1, 2], [1, 1]], requires_grad=True)\n",
    "b = Tensor([1, 1], requires_grad=True)\n",
    "y = W @ x + b\n",
    "label = Tensor([5, 5])\n",
    "loss = ((y - label) ** 2).sum()\n",
    "loss.backward()\n",
    "\n",
    "print('x:', x)\n",
    "print('W:', W)\n",
    "print('b:', b)\n",
    "print('y:', y)\n",
    "print('label:', label)\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da4e1672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add: [1.] [1.] (expected 1, 1)\n",
      "mul: [4.] [3.] (expected 4, 3)\n",
      "pow: [12.] (expected 12)\n",
      "div: [0.5] [-1.5] (expected 1/2, -6/4 = -1.5)\n",
      "exp: [7.389] (expected 7.389)\n",
      "log: [0.5] (expected 0.5)\n",
      "relu: [0. 1.] (expected [0, 1])\n",
      "sigmoid: [0.25] (expected 0.25)\n",
      "tanh: [1.] (expected 1.0)\n",
      "matmul: [[3. 4.]] [1. 2.] (expected [[3, 4]], [1, 2])\n",
      "sum: [1. 1. 1.] (expected [1, 1, 1])\n",
      "mean: [0.333 0.333 0.333] (expected [1/3, 1/3, 1/3])\n",
      "broadcast: [[3. 3.]] (expected [[3, 3]])\n",
      "reshape+transpose: [[1. 1.]\n",
      " [1. 1.]] (expected all ones)\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([3.0], requires_grad=True)\n",
    "b = Tensor([4.0], requires_grad=True)\n",
    "c = (a + b).sum()\n",
    "c.backward()\n",
    "print(\"add:\", a.grad, b.grad, \"(expected 1, 1)\")\n",
    "\n",
    "a = Tensor([3.0], requires_grad=True)\n",
    "b = Tensor([4.0], requires_grad=True)\n",
    "c = (a * b).sum()\n",
    "c.backward()\n",
    "print(\"mul:\", a.grad, b.grad, \"(expected 4, 3)\")\n",
    "\n",
    "x = Tensor([2.0], requires_grad=True)\n",
    "y = (x ** 3).sum()\n",
    "y.backward()\n",
    "print(\"pow:\", x.grad, \"(expected 12)\")\n",
    "\n",
    "a = Tensor([6.0], requires_grad=True)\n",
    "b = Tensor([2.0], requires_grad=True)\n",
    "c = (a / b).sum()\n",
    "c.backward()\n",
    "print(\"div:\", a.grad, b.grad, \"(expected 1/2, -6/4 = -1.5)\")\n",
    "\n",
    "x = Tensor([2.0], requires_grad=True)\n",
    "y = x.exp().sum()\n",
    "y.backward()\n",
    "print(\"exp:\", x.grad, \"(expected 7.389)\")\n",
    "\n",
    "x = Tensor([2.0], requires_grad=True)\n",
    "y = x.log().sum()\n",
    "y.backward()\n",
    "print(\"log:\", x.grad, \"(expected 0.5)\")\n",
    "\n",
    "x = Tensor([-1.0, 2.0], requires_grad=True)\n",
    "y = x.relu().sum()\n",
    "y.backward()\n",
    "print(\"relu:\", x.grad, \"(expected [0, 1])\")\n",
    "\n",
    "x = Tensor([0.0], requires_grad=True)\n",
    "y = x.sigmoid().sum()\n",
    "y.backward()\n",
    "print(\"sigmoid:\", x.grad, \"(expected 0.25)\")\n",
    "\n",
    "x = Tensor([0.0], requires_grad=True)\n",
    "y = x.tanh().sum()\n",
    "y.backward()\n",
    "print(\"tanh:\", x.grad, \"(expected 1.0)\")\n",
    "\n",
    "W = Tensor([[1.0, 2.0]], requires_grad=True)\n",
    "x = Tensor([3.0, 4.0], requires_grad=True)\n",
    "y = (W @ x).sum()\n",
    "y.backward()\n",
    "print(\"matmul:\", W.grad, x.grad, \"(expected [[3, 4]], [1, 2])\")\n",
    "\n",
    "x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "print(\"sum:\", x.grad, \"(expected [1, 1, 1])\")\n",
    "\n",
    "x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x.mean()\n",
    "y.backward()\n",
    "print(\"mean:\", x.grad, \"(expected [1/3, 1/3, 1/3])\")\n",
    "\n",
    "x = Tensor([[1.0, 2.0]], requires_grad=True)\n",
    "y = x.broadcast_to((3,2))\n",
    "z = y.sum()\n",
    "z.backward()\n",
    "print(\"broadcast:\", x.grad, \"(expected [[3, 3]])\")\n",
    "\n",
    "x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "y = x.reshape(4,)\n",
    "z = y.transpose().sum()\n",
    "z.backward()\n",
    "print(\"reshape+transpose:\", x.grad, \"(expected all ones)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
