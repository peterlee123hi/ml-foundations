{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328e4647",
   "metadata": {},
   "source": [
    "# Minigrad Autodiff Engine\n",
    "\n",
    "This notebook contains a minimal implementation of an automatic differentiation engine that supports reverse-mode autodiff on N-dimensional tensors:\n",
    "- Tensor: core data structure and computation graph\n",
    "- Broadcasting & reductions: broadcasting in forward pass and unbroadcasting in backward pass\n",
    "- Matmul: matrix multiplication and its vector-Jacobian products (VJPs)\n",
    "- NN module: neural network library with Linear layers, activations, parameter collection\n",
    "- Experiments: gradient checking, training simple models on toy datasets\n",
    "\n",
    "Note: this project is self-contained and won't be shared with CNN / Transformer / Diffusion, those projects will use Pytorch autograd.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc48969",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0451c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)\n",
    "np.random.seed(0)\n",
    "\n",
    "def _unbroadcast(grad, target_shape):\n",
    "    \"\"\"\n",
    "    Reduce a gradient to match the given target shape.\n",
    "\n",
    "    Params:\n",
    "        grad: np.array, the gradient with the broadcasted shape\n",
    "        target_shape: tuple of int, the shape of the tensor before broadcasting\n",
    "\n",
    "    Returns:\n",
    "        result: np.array, the gradient with reduced dimensions and summed so it matches the target shape\n",
    "    \"\"\"\n",
    "    # grad.ndim is number of axes, len(target_shape) is desired number of axes.\n",
    "    # For [[a, b], [c, d]], the result of sum(axis=0) is [a+c, b+d]\n",
    "    while grad.ndim > len(target_shape):\n",
    "        grad = grad.sum(axis=0)\n",
    "    \n",
    "    # For every axis where the original tensor had size 1 and was broadcasted, we sum\n",
    "    # the gradient along that axis to collapse it back.\n",
    "    for axis, dim in reversed(list(enumerate(target_shape))):\n",
    "        if dim == 1:\n",
    "            grad = grad.sum(axis=axis, keepdims=True)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfed5d6",
   "metadata": {},
   "source": [
    "## Tensor Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=False):\n",
    "        # Numpy scalar, vector, matrix\n",
    "        self.data = np.array(data, dtype=np.float64)\n",
    "\n",
    "        # Determines if we should store gradient\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        # Gradient of the loss wrt this tensor (same shape as .data)\n",
    "        self.grad = None\n",
    "\n",
    "        # Closure that computes local gradients and accumulates them into parent tensors\n",
    "        self._backward = lambda: None\n",
    "\n",
    "        # Parent tensors\n",
    "        self._prev = set()\n",
    "\n",
    "        # Op name for logging\n",
    "        self._op = ''\n",
    "\n",
    "    def __repr__(self):\n",
    "        # String representation for logging\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad}, op={self._op})\"\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Performs backpropagation on the computation graph and updates internal variables.\"\"\"\n",
    "        topo, visited = [], set()\n",
    "\n",
    "        # Build DFS topological sort of computation graph\n",
    "        def build(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for p in v._prev: \n",
    "                    build(p)\n",
    "                topo.append(v)\n",
    "        build(self)\n",
    "\n",
    "        # Set loss gradient to ones\n",
    "        self.grad = np.ones_like(self.data)\n",
    "\n",
    "        # Set computation graph gradients to zeros\n",
    "        for v in topo:\n",
    "            if v.requires_grad:\n",
    "                v.grad = np.zeros_like(v.data)\n",
    "\n",
    "        # Reverse topo sort order (starting from the outermost / loss layer)\n",
    "        for v in reversed(topo):\n",
    "            if v.requires_grad:\n",
    "                v._backward()\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"Elementwise addition with broadcasting, returns a new Tensor and sets up gradient flow to both parents\"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "\n",
    "        # Construct new tensor resulting from the operation\n",
    "        output_data = self.data + other.data\n",
    "        output = Tensor(data=output_data, requires_grad=(self.requires_grad or other.requires_grad))\n",
    "        output._prev = {self, other}\n",
    "        output._op = \"add\"\n",
    "\n",
    "        # For the new tensor, set backprop method to update the gradients of its parents.\n",
    "        # For addition, the local derivative with respect to each parent is 1, so the upstream\n",
    "        # gradient is passed straight through.\n",
    "        # .grad has the gradient with respect to the outermost layer.\n",
    "        def _backward():\n",
    "            output_grad = output.grad\n",
    "            for parent in output._prev:\n",
    "                if parent.requires_grad:\n",
    "                    adjusted_grad = _unbroadcast(output_grad, parent.data.shape)\n",
    "                    parent.grad = (parent.grad if parent.grad is not None else np.zeros_like(parent.data)) + adjusted_grad\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def __mul__(self, other): pass\n",
    "    def __matmul__(self, other): pass\n",
    "    def __neg__(self): pass\n",
    "    def __sub__(self, other): pass\n",
    "    def __truediv__(self, other): pass\n",
    "    def __pow__(self, power): pass\n",
    "\n",
    "    def exp(self): pass\n",
    "    def log(self): pass\n",
    "    def relu(self): pass\n",
    "    def sigmoid(self): pass\n",
    "    def tanh(self): pass\n",
    "\n",
    "    def sum(self, axis=None, keepdims=False): pass\n",
    "    def mean(self, axis=None, keepdims=False): pass\n",
    "\n",
    "    def reshape(self, *shape): pass\n",
    "    def transpose(self, *axes): pass\n",
    "    def broadcast_to(self, shape): pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
