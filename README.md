# ML Foundations

This is a living repository of practical projects for myself to revisit machine learning fundamentals and their underlying mathematical foundations. Each project is built with minimal dependencies (e.g., only NumPy).

## Educational Projects

Each project focuses on internalizing the mechanics.

### Deep Learning
- [ ] **Pure Python** neural network (forward + backprop)
- [ ] Backpropagation via matrix calculus (written derivation + implementation)
- [ ] CNN layers and training loops on toy data
- [ ] Attention mechanisms from scratch
- [ ] Minimal implementation of the Transformer architecture

### Mathematical Foundations
- [ ] Matrix decomposition toolkit (SVD, eigendecomposition, orthogonal projections)
- [ ] Gradient and Jacobian visualizer for multivariate functions
- [ ] Numerical optimization playground (gradient descent, SGD variants)
- [ ] Manual backprop walkthrough with symbolic derivation (LaTeX + markdown)

### Classical Machine Learning
- [ ] Logistic Regression classifier with loss surface plots and decision boundaries  
- [ ] Linear Regression from scratch with visualization + gradient descent  
- [ ] PCA from scratch with eigenvalue interpretation and 2D projection plots  
- [ ] SVM primal/dual solver with margin visualization  
- [ ] Decision Tree and Random Forest implementation with toy datasets

## Paper Reading Log

### Foundational Papers
- [ ] Attention is All You Need
- [ ] Auto-Encoding Variational Bayes
- [ ] Understanding Machine Learning: From Theory to Algorithms
- [ ] Deep Residual Learning for Image Recognition
- [ ] Understanding Deep Learning Requires Rethinking Generalization
- [ ] The Lottery Ticket Hypothesis
- [ ] Diffusion Models Beat GANs on Image Synthesis
- [ ] More to come...

## Tech Stack & Tools

- Python, NumPy, Matplotlib
- Jupyter Notebooks for visual explanation
- PyTorch (for sanity-checks only)
- Markdown + LaTeX for writeups
