# ML Foundations

This is a living repository of practical notebooks to revisit core machine learning concepts and the mathematical foundations behind them. Each project is kept minimal and transparent using mostly NumPy.

LLMs were used only to lightly improve written explanations - absolutely no LaTeX, code, or pseudocode was AI-generated.

## Educational Projects

### Deep Learning
- [ ] Backpropagation via matrix calculus (written derivation + implementation)
- [ ] Basic neural network from scratch (forward + backprop, ReLU, Leaky ReLU, sigmoid, tanh, GELU, Xavier/He initialization)
- [ ] CNN layers and training loops on toy data (BatchNorm, weight initialization, activation comparisons)
- [ ] Attention mechanisms from scratch (softmax temperature scaling, LayerNorm, positional encodings)
- [ ] Minimal implementation of the Transformer architecture (multi-head attention, residuals, LayerNorm, GELU)
- [ ] Variational Autoencoder (VAE) from scratch on MNIST (KL divergence, reparameterization, latent space interpolation)

### Mathematical Foundations
- [x] Matrix decomposition (SVD, eigendecomposition, orthogonal projections)
- [ ] Gradient and Jacobian visualizer for multivariate functions
- [ ] Numerical optimization (gradient descent, SGD variants)

### Classical Machine Learning
- [ ] Linear Regression from scratch (MLE, Bayesian inference, gradient descent, uncertainty visualization)
- [ ] Logistic Regression classifier with loss surface plots and decision boundaries  
- [ ] PCA from scratch with eigenvalue interpretation and 2D projection plots  
- [ ] SVM primal/dual solver with margin visualization  
- [ ] Decision Tree and Random Forest implementation with toy datasets

## Paper Reading Log

### Foundational Papers
- [ ] Attention is All You Need
- [ ] Auto-Encoding Variational Bayes
- [ ] Understanding Machine Learning: From Theory to Algorithms
- [ ] Deep Residual Learning for Image Recognition
- [ ] Understanding Deep Learning Requires Rethinking Generalization
- [ ] The Lottery Ticket Hypothesis
- [ ] Diffusion Models Beat GANs on Image Synthesis

## Tech Stack & Tools

- Python, NumPy, Matplotlib, Seaborn
- Jupyter Notebooks for derivations and code
- PyTorch, Scikit-learn (for sanity-checks only)
- Markdown + LaTeX for writeups
