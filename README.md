# ML Foundations

This is a living repository of practical notebooks to revisit core machine learning concepts and the mathematical foundations behind them. Each project is kept minimal and transparent, using only lightweight dependencies like NumPy.

LLMs were used only to improve written explanations - absolutely no code or pseudocode was AI-generated.

## Educational Projects

### Deep Learning
- [ ] Basic neural network from scratch (forward + backprop)
- [ ] Backpropagation via matrix calculus (written derivation + implementation)
- [ ] CNN layers and training loops on toy data
- [ ] Attention mechanisms from scratch
- [ ] Minimal implementation of the Transformer architecture
- [ ] Variational Autoencoder (VAE) from scratch on MNIST

### Mathematical Foundations
- [ ] Matrix decomposition (SVD, eigendecomposition, orthogonal projections)
- [ ] Gradient and Jacobian visualizer for multivariate functions
- [ ] Numerical optimization (gradient descent, SGD variants)

### Classical Machine Learning
- [ ] Logistic Regression classifier with loss surface plots and decision boundaries  
- [ ] Linear Regression from scratch with visualization + gradient descent  
- [ ] PCA from scratch with eigenvalue interpretation and 2D projection plots  
- [ ] SVM primal/dual solver with margin visualization  
- [ ] Decision Tree and Random Forest implementation with toy datasets

## Paper Reading Log

### Foundational Papers
- [ ] Attention is All You Need
- [ ] Auto-Encoding Variational Bayes
- [ ] Understanding Machine Learning: From Theory to Algorithms
- [ ] Deep Residual Learning for Image Recognition
- [ ] Understanding Deep Learning Requires Rethinking Generalization
- [ ] The Lottery Ticket Hypothesis
- [ ] Diffusion Models Beat GANs on Image Synthesis
- [ ] More to come...

## Tech Stack & Tools

- Python, NumPy, Matplotlib, Seaborn
- Colab Notebooks for visual explanation
- PyTorch, Scikit-learn (for sanity-checks only)
- Markdown + LaTeX for writeups
