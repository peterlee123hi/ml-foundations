{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d426b9b",
   "metadata": {},
   "source": [
    "# Matrix Decomposition\n",
    "\n",
    "This notebook covers common matrix decomposition methods used in machine learning:\n",
    "- QR Decomposition\n",
    "- Eigen Decomposition\n",
    "- Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc09a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bedd0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      " [[1. 0.]\n",
      " [0. 1.]]\n",
      "R: \n",
      " [[1. 2.]\n",
      " [0. 2.]]\n",
      "\n",
      "===\n",
      "\n",
      "Q: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "R: \n",
      " [[1. 1. 0.]\n",
      " [0. 1. 1.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "===\n",
      "\n",
      "Q: \n",
      " [[ 0.70710678  0.40824829 -0.57735027]\n",
      " [ 0.70710678 -0.40824829  0.57735027]\n",
      " [ 0.          0.81649658  0.57735027]]\n",
      "R: \n",
      " [[1.41421356 0.70710678 0.70710678]\n",
      " [0.         1.22474487 0.40824829]\n",
      " [0.         0.         1.15470054]]\n"
     ]
    }
   ],
   "source": [
    "def qr_decomposition(vectors):\n",
    "    \"\"\"\n",
    "    Perform QR decomposition via Gram-Schmidt on a set of vectors\n",
    "\n",
    "    Args:\n",
    "        vectors: A numpy matrix n by m, where each column is a vector of length n\n",
    "\n",
    "    Returns:\n",
    "        Q, R where Q = orthonormal basis and R = upper triangular matrix of scalar projections\n",
    "        and A = Q * R\n",
    "    \"\"\"\n",
    "    n, m = vectors.shape\n",
    "    A = vectors.copy().astype(float)\n",
    "    Q = np.zeros((n, m)).astype(float)\n",
    "    R = np.zeros((m, m)).astype(float)\n",
    "\n",
    "    # We can normalize beforehand to simplify projecting each vector onto every other vector\n",
    "    # But that may lead to floating point errors, so let's prefer the standard method\n",
    "    for j1 in range(m):\n",
    "        vec_j1 = A[:, j1].copy()\n",
    "\n",
    "        # Subtract projections of vec_j1 on previous vectors\n",
    "        ortho_vec_j1 = A[:, j1].copy()\n",
    "        for j2 in range(j1):\n",
    "            vec_j2 = Q[:, j2].copy()\n",
    "\n",
    "            # Normalize vec_j2\n",
    "            mag_j2 = np.sqrt(np.sum(vec_j2 * vec_j2))\n",
    "            norm_vec_j2 = vec_j2 / mag_j2\n",
    "\n",
    "            # Get the scalar projection of vec_j1 onto vec_j2:\n",
    "            # The scalar projection is typically vec_j1 @ vec_j2 / ||vec_j2||\n",
    "            # But we have the normalized vector of vec_j2\n",
    "            # And vec_j1 @ norm_vec_j2 = ||vec_j1|| * ||norm_vec_j2|| * cos(theta) = ||vec_j1|| * cos(theta)\n",
    "            # Because vec_j1 is the hypotenus and cos(theta) = adj / hyp\n",
    "            # Thus, ||vec_j1|| * cos(theta) = length of adj = length of vec_j1 projected onto norm_vec_j2\n",
    "            # \"A unit vectorâ€™s components are the cosines of its angles with the coordinate axes.\"\n",
    "            scalar_proj_j1 = np.dot(vec_j1, norm_vec_j2)\n",
    "            \n",
    "            # Store the scalar projection of the original vector onto norm_vec_j2\n",
    "            R[j2][j1] = scalar_proj_j1\n",
    "\n",
    "            # Project vec_j1 onto vec_j2\n",
    "            proj_j1_onto_j2 = scalar_proj_j1 * norm_vec_j2\n",
    "\n",
    "            # Subtract the projection from ortho_vec_j1\n",
    "            ortho_vec_j1 -= proj_j1_onto_j2\n",
    "\n",
    "        # Normalize ortho_vec_j1\n",
    "        mag_ortho_j1 = np.sqrt(np.sum(ortho_vec_j1 * ortho_vec_j1))\n",
    "        orthonorm_vec_j1 = ortho_vec_j1 / mag_ortho_j1\n",
    "        R[j1][j1] = mag_ortho_j1\n",
    "\n",
    "        # Add orthonormal basis vector to Q\n",
    "        Q[:, j1] = orthonorm_vec_j1\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "sample_vectors = np.array([[1, 2], \n",
    "                           [0, 2]])\n",
    "Q, R = qr_decomposition(sample_vectors)\n",
    "print('Q: \\n', Q)\n",
    "print('R: \\n', R)\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "sample_vectors = np.array([[1, 1, 0], \n",
    "                           [0, 1, 1], \n",
    "                           [0, 0, 1]])\n",
    "Q, R = qr_decomposition(sample_vectors)\n",
    "print('Q: \\n', Q)\n",
    "print('R: \\n', R)\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "sample_vectors = np.array([[1, 1, 0], \n",
    "                           [1, 0, 1], \n",
    "                           [0, 1, 1]])\n",
    "Q, R = qr_decomposition(sample_vectors)\n",
    "print('Q: \\n', Q)\n",
    "print('R: \\n', R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "582cff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "lambdas: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "V_inverse: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "V * lambdas * V_inverse: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "===\n",
      "\n",
      "V: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "lambdas: \n",
      " [[2. 0. 0.]\n",
      " [0. 3. 0.]\n",
      " [0. 0. 4.]]\n",
      "V_inverse: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "V * lambdas * V_inverse: \n",
      " [[2. 0. 0.]\n",
      " [0. 3. 0.]\n",
      " [0. 0. 4.]]\n",
      "\n",
      "===\n",
      "\n",
      "V: \n",
      " [[-5.00000000e-01  7.07106781e-01  5.00000000e-01]\n",
      " [-7.07106781e-01  4.05405432e-16 -7.07106781e-01]\n",
      " [-5.00000000e-01 -7.07106781e-01  5.00000000e-01]]\n",
      "lambdas: \n",
      " [[3.41421356 0.         0.        ]\n",
      " [0.         2.         0.        ]\n",
      " [0.         0.         0.58578644]]\n",
      "V_inverse: \n",
      " [[-5.00000000e-01 -7.07106781e-01 -5.00000000e-01]\n",
      " [ 7.07106781e-01  3.14018492e-16 -7.07106781e-01]\n",
      " [ 5.00000000e-01 -7.07106781e-01  5.00000000e-01]]\n",
      "V * lambdas * V_inverse: \n",
      " [[ 2.  1. -0.]\n",
      " [ 1.  2.  1.]\n",
      " [-0.  1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "def eigendecomposition(A):\n",
    "    \"\"\"\n",
    "    Performs eigendecomposition on a square matrix A\n",
    "\n",
    "    Args:\n",
    "        A: A 2D np.array (n, n), the square matrix to decompose\n",
    "\n",
    "    Returns:\n",
    "        V,  where V = eigenvectors and R = diagonalized matrix of eigenvalues\n",
    "        and A = V * lambdas * V^-1\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    V = np.zeros((n, n)).astype(float)\n",
    "    lambdas = np.zeros((n, n)).astype(float)\n",
    "    V_inverse = np.zeros((n, n)).astype(float)\n",
    "\n",
    "    # Getting eigenvalues by hand means getting the roots of the characteristic polynomial\n",
    "    # for A - lambda * I. Eigenvectors of a linear transformation are vectors that are on the same span\n",
    "    # after applying the transformation and the eigenvalues are the multiplier that they scale by. Or\n",
    "    # mathematically, vectors such that A * v = lambda * v. For each eigenvalue, you want to find the \n",
    "    # null space of A - lambda * I that gives you the eigenvectors.\n",
    "    eigenvals, eigenvecs = np.linalg.eig(A)\n",
    "\n",
    "    for i in range(n):\n",
    "        V[:,i] = eigenvecs[:,i]\n",
    "        lambdas[i][i] = eigenvals[i]\n",
    "\n",
    "    V_inverse = np.linalg.inv(V)\n",
    "\n",
    "    return V, lambdas, V_inverse\n",
    "\n",
    "sample_A = np.array([[1, 0, 0], \n",
    "                           [0, 1, 0], \n",
    "                           [0, 0, 1]])\n",
    "V, lambdas, V_inverse = eigendecomposition(sample_A)\n",
    "print('V: \\n', V)\n",
    "print('lambdas: \\n', lambdas)\n",
    "print('V_inverse: \\n', V_inverse)\n",
    "print('V * lambdas * V_inverse: \\n', V @ lambdas @ V_inverse)\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "sample_A = np.array([[2, 0, 0], \n",
    "                           [0, 3, 0], \n",
    "                           [0, 0, 4]])\n",
    "V, lambdas, V_inverse = eigendecomposition(sample_A)\n",
    "print('V: \\n', V)\n",
    "print('lambdas: \\n', lambdas)\n",
    "print('V_inverse: \\n', V_inverse)\n",
    "print('V * lambdas * V_inverse: \\n', V @ lambdas @ V_inverse)\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "sample_A = np.array([[2, 1, 0], \n",
    "                           [1, 2, 1], \n",
    "                           [0, 1, 2]])\n",
    "V, lambdas, V_inverse = eigendecomposition(sample_A)\n",
    "print('V: \\n', V)\n",
    "print('lambdas: \\n', lambdas)\n",
    "print('V_inverse: \\n', V_inverse)\n",
    "print('V * lambdas * V_inverse: \\n', np.round(V @ lambdas @ V_inverse, decimals=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
