{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d426b9b",
   "metadata": {},
   "source": [
    "# Matrix Decomposition\n",
    "\n",
    "This notebook covers common matrix decomposition methods used in machine learning:\n",
    "- QR Decomposition\n",
    "- Eigen Decomposition\n",
    "- Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f34ac1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc09a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d435215c",
   "metadata": {},
   "source": [
    "## QR Decomposition\n",
    "\n",
    "Decompose a matrix or set of vectors into an upper triangular matrix and an orthnormal basis using Gram-Schmidt.\n",
    "\n",
    "$$A  = Q R$$\n",
    "\n",
    "Where:\n",
    "- $A \\in \\mathbb{R}^{n \\times m}$\n",
    "- $Q \\in \\mathbb{R}^{n \\times m}$ is an orthonormal basis\n",
    "- $R \\in \\mathbb{R}^{m \\times m}$ is an upper triangular matrix of scalar projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bedd0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \n",
      " [[1. 0.]\n",
      " [0. 1.]]\n",
      "R: \n",
      " [[1. 2.]\n",
      " [0. 2.]]\n",
      "A: \n",
      " [[1. 2.]\n",
      " [0. 2.]]\n",
      "\n",
      "===\n",
      "\n",
      "Q: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "R: \n",
      " [[1. 1. 0.]\n",
      " [0. 1. 1.]\n",
      " [0. 0. 1.]]\n",
      "A: \n",
      " [[1. 1. 0.]\n",
      " [0. 1. 1.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "===\n",
      "\n",
      "Q: \n",
      " [[ 0.70710678  0.40824829 -0.57735027]\n",
      " [ 0.70710678 -0.40824829  0.57735027]\n",
      " [ 0.          0.81649658  0.57735027]]\n",
      "R: \n",
      " [[1.41421356 0.70710678 0.70710678]\n",
      " [0.         1.22474487 0.40824829]\n",
      " [0.         0.         1.15470054]]\n",
      "A: \n",
      " [[ 1.  1. -0.]\n",
      " [ 1. -0.  1.]\n",
      " [ 0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "def qr_decompose(A):\n",
    "    \"\"\"\n",
    "    Perform QR decomposition via Gram-Schmidt on a set of vectors\n",
    "\n",
    "    Args:\n",
    "        A: A numpy matrix n by m, where each column is a vector of length n\n",
    "\n",
    "    Returns:\n",
    "        Q: an n by m matrix that is an orthonormal basis\n",
    "        R: an m by m upper triangular matrix of scalar projections\n",
    "    \"\"\"\n",
    "    n, m = A.shape\n",
    "    A = A.copy().astype(float)\n",
    "    Q = np.zeros((n, m)).astype(float)\n",
    "    R = np.zeros((m, m)).astype(float)\n",
    "\n",
    "    # We can normalize beforehand to simplify projecting each vector onto every other vector\n",
    "    # But that may lead to floating point errors, so let's prefer the standard method\n",
    "    for j1 in range(m):\n",
    "        vec_j1 = A[:, j1].copy()\n",
    "\n",
    "        # Subtract projections of vec_j1 on previous vectors\n",
    "        ortho_vec_j1 = A[:, j1].copy()\n",
    "        for j2 in range(j1):\n",
    "            vec_j2 = Q[:, j2].copy()\n",
    "\n",
    "            # Normalize vec_j2\n",
    "            mag_j2 = np.sqrt(np.sum(vec_j2 * vec_j2))\n",
    "            norm_vec_j2 = vec_j2 / mag_j2\n",
    "\n",
    "            # Get the scalar projection of vec_j1 onto vec_j2:\n",
    "            # The scalar projection is typically vec_j1 @ vec_j2 / ||vec_j2||\n",
    "            # But we have the normalized vector of vec_j2\n",
    "            # And vec_j1 @ norm_vec_j2 = ||vec_j1|| * ||norm_vec_j2|| * cos(theta) = ||vec_j1|| * cos(theta)\n",
    "            # Because vec_j1 is the hypotenus and cos(theta) = adj / hyp\n",
    "            # Thus, ||vec_j1|| * cos(theta) = length of adj = length of vec_j1 projected onto norm_vec_j2\n",
    "            # \"A unit vectorâ€™s components are the cosines of its angles with the coordinate axes.\"\n",
    "            scalar_proj_j1 = np.dot(vec_j1, norm_vec_j2)\n",
    "            \n",
    "            # Store the scalar projection of the original vector onto norm_vec_j2\n",
    "            R[j2][j1] = scalar_proj_j1\n",
    "\n",
    "            # Project vec_j1 onto vec_j2\n",
    "            proj_j1_onto_j2 = scalar_proj_j1 * norm_vec_j2\n",
    "\n",
    "            # Subtract the projection from ortho_vec_j1\n",
    "            ortho_vec_j1 -= proj_j1_onto_j2\n",
    "\n",
    "        # Normalize ortho_vec_j1\n",
    "        mag_ortho_j1 = np.sqrt(np.sum(ortho_vec_j1 * ortho_vec_j1))\n",
    "        orthonorm_vec_j1 = ortho_vec_j1 / mag_ortho_j1\n",
    "        R[j1][j1] = mag_ortho_j1\n",
    "\n",
    "        # Add orthonormal basis vector to Q\n",
    "        Q[:, j1] = orthonorm_vec_j1\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "sample_vectors = np.array([[1, 2], \n",
    "                           [0, 2]])\n",
    "Q, R = qr_decompose(sample_vectors)\n",
    "print('Q: \\n', Q)\n",
    "print('R: \\n', R)\n",
    "print('A: \\n', Q @ R)\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "sample_vectors = np.array([[1, 1, 0], \n",
    "                           [0, 1, 1], \n",
    "                           [0, 0, 1]])\n",
    "Q, R = qr_decompose(sample_vectors)\n",
    "print('Q: \\n', Q)\n",
    "print('R: \\n', R)\n",
    "print('A: \\n', Q @ R)\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "sample_vectors = np.array([[1, 1, 0], \n",
    "                           [1, 0, 1], \n",
    "                           [0, 1, 1]])\n",
    "Q, R = qr_decompose(sample_vectors)\n",
    "print('Q: \\n', Q)\n",
    "print('R: \\n', R)\n",
    "print('A: \\n', np.round(Q @ R, decimals=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677abaf",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "QR decomp takes a set of vectors and expresses them as combinations of orthonormal basis vectors and scalar projections. But eigendecompose operates on a linear transformation as a whole by expressing the matrix in terms of its eigenvalues and eigenvectors. In the basis of eigenvectors, the linear transformation scales each eigenvector by the eigenvalue so we see how the linear transformation modifies the space in the direction of each eigenvector.\n",
    "\n",
    "$$A = V \\Lambda V^{-1}$$\n",
    "\n",
    "Where:\n",
    "- $A \\in \\mathbb{R}^{n \\times n}$ is a square matrix that is diagonalizable (has basis of eigenvectors)\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ is a matrix whose columns are the eigenvectors of $A$\n",
    "- $\\Lambda \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$ corresponding to the eigenvectors in $V$\n",
    "- $V^{-1}$ is the inverse of the eigenvector matrix $V$ (only exists if the eigenvectors are linearly independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582cff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "lambdas: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "V_inverse: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "V * lambdas * V_inverse: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "===\n",
      "\n",
      "V: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "lambdas: \n",
      " [[2. 0. 0.]\n",
      " [0. 3. 0.]\n",
      " [0. 0. 4.]]\n",
      "V_inverse: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "V * lambdas * V_inverse: \n",
      " [[2. 0. 0.]\n",
      " [0. 3. 0.]\n",
      " [0. 0. 4.]]\n",
      "\n",
      "===\n",
      "\n",
      "V: \n",
      " [[-5.00000000e-01  7.07106781e-01  5.00000000e-01]\n",
      " [-7.07106781e-01  4.05405432e-16 -7.07106781e-01]\n",
      " [-5.00000000e-01 -7.07106781e-01  5.00000000e-01]]\n",
      "lambdas: \n",
      " [[3.41421356 0.         0.        ]\n",
      " [0.         2.         0.        ]\n",
      " [0.         0.         0.58578644]]\n",
      "V_inverse: \n",
      " [[-5.00000000e-01 -7.07106781e-01 -5.00000000e-01]\n",
      " [ 7.07106781e-01  3.14018492e-16 -7.07106781e-01]\n",
      " [ 5.00000000e-01 -7.07106781e-01  5.00000000e-01]]\n",
      "V * lambdas * V_inverse: \n",
      " [[ 2.  1. -0.]\n",
      " [ 1.  2.  1.]\n",
      " [-0.  1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "def eigendecompose(A):\n",
    "    \"\"\"\n",
    "    Performs eigendecompose on a square matrix A\n",
    "\n",
    "    Args:\n",
    "        A: A numpy matrix n by n to decompose\n",
    "\n",
    "    Returns:\n",
    "        V: an n by n matrix of eigenvectors\n",
    "        lambdas: diagonalized matrix of eigenvalues\n",
    "        V_inverse: the inverse of V\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    V = np.zeros((n, n)).astype(float)\n",
    "    lambdas = np.zeros((n, n)).astype(float)\n",
    "    V_inverse = np.zeros((n, n)).astype(float)\n",
    "\n",
    "    # Getting eigenvalues by hand means getting the roots of the characteristic polynomial\n",
    "    # for A - lambda * I. Eigenvectors of a linear transformation are vectors that are on \n",
    "    # the same span after applying the transformation and the eigenvalues are the multiplier \n",
    "    # that they scale by. Or mathematically, vectors such that A * v = lambda * v. For each \n",
    "    # eigenvalue, you want to find the null space of A - lambda * I that gives you the \n",
    "    # eigenvectors.\n",
    "    eigenvals, eigenvecs = np.linalg.eig(A)\n",
    "\n",
    "    for i in range(n):\n",
    "        V[:, i] = eigenvecs[:, i]\n",
    "        lambdas[i][i] = eigenvals[i]\n",
    "\n",
    "    V_inverse = np.linalg.inv(V)\n",
    "\n",
    "    return V, lambdas, V_inverse\n",
    "\n",
    "sample_A = np.array([[1, 0, 0], \n",
    "                     [0, 1, 0], \n",
    "                     [0, 0, 1]])\n",
    "V, lambdas, V_inverse = eigendecompose(sample_A)\n",
    "print('V: \\n', V)\n",
    "print('lambdas: \\n', lambdas)\n",
    "print('V_inverse: \\n', V_inverse)\n",
    "print('V * lambdas * V_inverse: \\n', V @ lambdas @ V_inverse)\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "sample_A = np.array([[2, 0, 0], \n",
    "                     [0, 3, 0], \n",
    "                     [0, 0, 4]])\n",
    "V, lambdas, V_inverse = eigendecompose(sample_A)\n",
    "print('V: \\n', V)\n",
    "print('lambdas: \\n', lambdas)\n",
    "print('V_inverse: \\n', V_inverse)\n",
    "print('V * lambdas * V_inverse: \\n', V @ lambdas @ V_inverse)\n",
    "\n",
    "print('\\n===\\n')\n",
    "\n",
    "sample_A = np.array([[2, 1, 0], \n",
    "                     [1, 2, 1], \n",
    "                     [0, 1, 2]])\n",
    "V, lambdas, V_inverse = eigendecompose(sample_A)\n",
    "print('V: \\n', V)\n",
    "print('lambdas: \\n', lambdas)\n",
    "print('V_inverse: \\n', V_inverse)\n",
    "print('V * lambdas * V_inverse: \\n', np.round(V @ lambdas @ V_inverse, decimals=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b7803",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition\n",
    "\n",
    "So SVD is a generalized form of eigendecompose that applies to any linear transformation that maps vectors from an input space of rank $m$ to an output space of rank $n$. It decomposes a transformation into the rotation in the input space (domain basis), scaling along new axes (singlular values), and rotation into the output space (range basis). The computation itself doesn't seem to have a nice geometric intuition compared to the final components of SVD. But the algebraic derivation from the formula involves transposes of orthogonal bases and that $A * v_i = \\epsilon_i * u_i$ where $v_i$ is in the input space and $u_i$ is in the output space, which is oddly similar to another concept.\n",
    "\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "Where:\n",
    "- $A \\in \\mathbb{R}^{m \\times n}$ is any real matrix (square or rectangular)\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns are the left singular vectors of $A$\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix (with non-negative real numbers) containing the singular values of $A$ on the diagonal, sorted in decreasing order\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix whose columns are the right singular vectors of $A$\n",
    "- $V^T$ is the transpose of the right singular vector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72449199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:\n",
      " [[ 0.57735027 -0.70710678 -0.40824829]\n",
      " [ 0.57735027  0.          0.81649658]\n",
      " [ 0.57735027  0.70710678 -0.40824829]]\n",
      "E:\n",
      " [[4.89897949 0.        ]\n",
      " [0.         2.        ]\n",
      " [0.         0.        ]]\n",
      "V:\n",
      " [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "U @ E @ V.T:\n",
      " [[3. 1.]\n",
      " [2. 2.]\n",
      " [1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "def svd(A):\n",
    "    \"\"\"\n",
    "    Performs singular value decomposition on an m x n matrix A\n",
    "\n",
    "    Args:\n",
    "        A: A numpy matrix m by n to decompose\n",
    "\n",
    "    Returns:\n",
    "        U: an m by m orthogonal matrix with left singular values\n",
    "        E: an m by n diagonal matrix with non-negative singular values\n",
    "        V: an n by n orthogonal matrix with right singular values\n",
    "        where A = U @ E @ V^T\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    U = np.zeros((m, m))\n",
    "    E = np.zeros((m, n))\n",
    "    V = np.zeros((n, n))\n",
    "\n",
    "    # A^T @ A = V @ E @ V^T\n",
    "    # We need to eigendecompose the gram matrix.\n",
    "    gram = np.transpose(A) @ A\n",
    "    eigenvals, eigenvecs = np.linalg.eigh(gram)\n",
    "\n",
    "    # We want to sort by decreasing eigenvalue to make it easier afterwards when we\n",
    "    # want to generate U, we can leave all the eigenvalues of 0 at the end\n",
    "    indices = np.argsort(eigenvals)[::-1]\n",
    "    eigenvals = eigenvals[indices]\n",
    "    eigenvecs = eigenvecs[:, indices]\n",
    "    for i in range(n):\n",
    "        # Eigenvectors are already normalized\n",
    "        V[:, i] = eigenvecs[:, i]\n",
    "\n",
    "    # Singular vals are sqrt(eigenvals of A^T @ A)\n",
    "    singular_vals = np.sqrt(eigenvals)\n",
    "    for i in range(min(m, n)):\n",
    "        E[i, i] = singular_vals[i]\n",
    "\n",
    "    # Generate u_i = (1 / singular_val_i) * (A @ v_i)\n",
    "    r = min(len([singular_val for singular_val in singular_vals if singular_val > 1e-10]), m)\n",
    "    for i in range(r):\n",
    "        v_i = V[:, i]\n",
    "        u_i = (1 / singular_vals[i]) * (A @ v_i)\n",
    "        U[:, i] = u_i\n",
    "\n",
    "    # Fill in the rest of U with any orthonormal basis\n",
    "    Q = U[:, :r]\n",
    "    candidates = np.random.randn(m, m - r)\n",
    "    for i in range(r):\n",
    "        q_i = Q[:, i]\n",
    "        for j in range(m - r):\n",
    "            # q_i is already a unit vec so the dot product should be the scalar projection\n",
    "            proj = np.dot(q_i, candidates[:, j])\n",
    "\n",
    "            # We can subtract the projection from the candidates\n",
    "            candidates[:, j] -= proj * q_i\n",
    "        \n",
    "    # Orthonormalize the resulting columns\n",
    "    Q_new, _ = qr_decompose(candidates)\n",
    "    U[:, r:] = Q_new\n",
    "\n",
    "    return U, E, V\n",
    "\n",
    "sample_A = np.array([[3, 1],\n",
    "                     [2, 2],\n",
    "                     [1, 3]], dtype=float)\n",
    "\n",
    "U, E, V = svd(sample_A)\n",
    "\n",
    "print('U:\\n', U)\n",
    "print('E:\\n', E)\n",
    "print('V:\\n', V)\n",
    "print('U @ E @ V.T:\\n', np.round(U @ E @ V.T, decimals=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
