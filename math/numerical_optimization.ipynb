{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d8758a",
   "metadata": {},
   "source": [
    "# Numerical Optimization\n",
    "\n",
    "This notebook introduces numerical optimization methods in machine learning that minimize loss functions efficiently and stably. The focus is on understanding the intuition, math, and convergence behavior of common optimizers:\n",
    "- Gradient descent and learning rate schedules\n",
    "- Stochastic gradient descent (SGD) and mini-batching\n",
    "- Momentum accelerated gradients\n",
    "- Adam optimizer (adaptive method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545145ce",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836eb2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada1432",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797c869",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb7dbc",
   "metadata": {},
   "source": [
    "## Momentum Accelerated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb749429",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
