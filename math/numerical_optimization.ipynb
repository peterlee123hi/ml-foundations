{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d8758a",
   "metadata": {},
   "source": [
    "# Numerical Optimization\n",
    "\n",
    "This notebook introduces numerical optimization methods in machine learning that minimize loss functions efficiently and stably. The focus is on understanding the intuition, math, and convergence behavior of common optimizers:\n",
    "- Gradient descent and learning rate schedules\n",
    "- Stochastic gradient descent (SGD) and mini-batching\n",
    "- Momentum and Nesterov accelerated gradients\n",
    "- Adaptive methods (AdaGrad, RMSProp, Adam)\n",
    "- Convergence analysis and visulization of loss surfaces for each algorithm"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
