{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d8758a",
   "metadata": {},
   "source": [
    "# Numerical Optimization\n",
    "\n",
    "This notebook introduces numerical optimization methods in machine learning that minimize loss functions efficiently and stably. The focus is on understanding the intuition, math, and convergence behavior of common optimizers:\n",
    "- Gradient descent and learning rate schedules\n",
    "- Stochastic gradient descent (SGD) and mini-batching\n",
    "- Momentum accelerated gradients\n",
    "- Adam optimizer (adaptive method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545145ce",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836eb2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "def quad_fn(x, A, b, c):\n",
    "    \"\"\"\n",
    "    f(x) = 0.5 x^T A x + b^T x + c\n",
    "    \n",
    "    x: (n,)\n",
    "    A: (n, n) symmetric matrix\n",
    "    b: (n,)\n",
    "    c: scalar\n",
    "    \"\"\"\n",
    "    return 0.5 * x @ A @ x + b @ x + c\n",
    "\n",
    "def quad_grad(x, A, b):\n",
    "    \"\"\"\n",
    "    grad_f(x) = A x + b\n",
    "\n",
    "    x: (n,)\n",
    "    A: (n, n)\n",
    "    b: (n,)\n",
    "    \"\"\"\n",
    "    return A @ x + b\n",
    "\n",
    "def rosenbrock_fn(x, y, a, b):\n",
    "    \"\"\"\n",
    "    f(x, y) = (a - x)^2 + b(y - x^2)^2\n",
    "\n",
    "    x: scalar\n",
    "    y: scalar\n",
    "    a: scalar\n",
    "    b: scalar\n",
    "    \"\"\"\n",
    "    return (a - x)**2 + b * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_grad(x, y, a, b):\n",
    "    \"\"\"\n",
    "    grad_f(x, y) = [df/dx, df/dy]\n",
    "    df/dx = -2(a - x) - 4b x (y - x^2)\n",
    "    df/dy = 2b (y - x^2)\n",
    "    \n",
    "    x: scalar\n",
    "    y: scalar\n",
    "    a: scalar\n",
    "    b: scalar\n",
    "    \"\"\"\n",
    "    dfdx = -2 * (a - x) - 4 * b * x * (y - x**2)\n",
    "    dfdy = 2 * b * (y - x**2)\n",
    "    return np.array([dfdx, dfdy])\n",
    "\n",
    "def saddle_fn(x, y):\n",
    "    \"\"\"\n",
    "    f(x, y) = x^2 - y^2\n",
    "\n",
    "    x: scalar\n",
    "    y: scalar\n",
    "    \"\"\"\n",
    "    return x**2 - y**2\n",
    "\n",
    "def saddle_grad(x, y):\n",
    "    \"\"\"\n",
    "    grad_f(x, y) = [df/dx, df/dy]\n",
    "    df/dx = 2x\n",
    "    df/dy = -2y\n",
    "\n",
    "    x: scalar\n",
    "    y: scalar\n",
    "    \"\"\"\n",
    "    return np.array([2 * x, -2 * y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada1432",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797c869",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb7dbc",
   "metadata": {},
   "source": [
    "## Momentum Accelerated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb749429",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
