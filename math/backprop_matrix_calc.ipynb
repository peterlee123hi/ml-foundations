{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e2f755",
   "metadata": {},
   "source": [
    "# Backpropagation via Matrix Calculus\n",
    "\n",
    "We will provide written matrix calculus derivations and the implementation for a reverse-mode autodiff engine, then use it to implement and verify backpropagation across common layers and losses:\n",
    "- Derive vector and matrix form gradients using differentials, the trace trick, and chain rules\n",
    "- Implement a reverse-mode autodiff core (vector-Jacobian products)\n",
    "- Derive and code backprop for: affine $\\rightarrow$ activation $\\rightarrow$ softmax-CE; MSE regression; L2 regularization\n",
    "- Verify gradients with high-precision finite differences on randomized cases\n",
    "- Show performance wins of reverse vs forward mode on wide nets (empirical scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88af6f4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890bc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc53254",
   "metadata": {},
   "source": [
    "## Matrix Calculus Basics\n",
    "\n",
    "**Intro**\n",
    "- $\\Delta$: big, finite step (difference)\n",
    "- $d$: tiny, infinitesimal step (differential)\n",
    "- $\\partial$: derivative with respect to one variable (partial)\n",
    "- $\\nabla$: gradient operator (vector of partials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8180f2",
   "metadata": {},
   "source": [
    "**Differentials**\n",
    "\n",
    "$d(\\cdot)$ means a differential, i.e. the infinitesimal change in a function when its inputs change a little. For a scalar function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$,\n",
    "\n",
    "$$df(x) = \\nabla f(x)^T dx \\approx f(x + \\Delta x) - f(x)$$\n",
    "\n",
    "where:\n",
    "- $dx$ is the vector of infinitesimal input changes\n",
    "- $\\nabla f(x)$ is the gradient (rate of change of $f$ with respect to each coordinate)\n",
    "\n",
    "So $d f(x)$ is the infinitesimal change in the output, predicted linearly from the gradient and input change.\n",
    "\n",
    "Let's consider the scalar function $f(x, y) = x^T Ay$. Let's see how $f$ changes when $x \\mapsto x + dx, y \\mapsto y + dy$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "d f(x, y) &= d(x^T Ay) \\\\\n",
    "&= (x + dx)^T A (y + dy) - x^T Ay \\\\\n",
    "&= dx^T Ay + x^T Ady + dx^T Ady \\\\\n",
    "&= (Ay)^T dx + (A^T x)^T dy\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We drop $dx^T Ady$ because it is the product of two infinitesimals.\n",
    "\n",
    "So the syntax $d(x^T Ay) = (Ay)^T dx + (A^T x)^T dy$ is a compact way of saying:\n",
    "- the gradient wrt $x$ is $Ay$\n",
    "- the gradient wrt $y$ is $A^T x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb7ea4",
   "metadata": {},
   "source": [
    "**Trace trick**\n",
    "\n",
    "The trace of a square matrix is the sum of the diagonal elements.\n",
    "\n",
    "$$\\text{tr}(M) = \\sum_i M_{ii}$$\n",
    "\n",
    "The trace has a cyclic property meaning $\\text{tr}(ABC) = \\text{tr}(BCA) = \\text{tr}(CAB)$ and linearity meaning $\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)$.\n",
    "\n",
    "Gradients are sometimes easiest to express in terms of traces. The differential of a scalar function $f(X)$ with matrix input is often written:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "df &= \\sum_{i, j} \\frac{\\partial f}{\\partial X_{ij}} dX_{ij} \\\\\n",
    "&= \\text{tr}((\\frac{\\partial f}{\\partial X})^T dX) \\\\\n",
    "&= \\text{tr}(G^T dX)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $G$ is the gradient $\\frac{\\partial f}{\\partial X}$.\n",
    "\n",
    "For example, if we take the scalar function $f(X) = tr(A^T X)$, then its differential is $df = \\text{tr}(A^T dX)$ and the gradient matrix with respect to $X$ is $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9b8e9",
   "metadata": {},
   "source": [
    "**Quadratic forms**\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{N \\times d}$, $W \\in \\mathbb{R}^{d \\times k}$, and $Y \\in \\mathbb{R}^{N \\times k}$. Define the loss:\n",
    "\n",
    "$$f(X, W) = \\frac{1}{2} \\|XW - Y\\|_F^2 = \\frac{1}{2}\\text{tr}((XW - Y)^T (XW - Y))$$\n",
    "\n",
    "Differential:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "df &= \\text{tr}((XW - Y)^T (dX \\, W + X \\, dW)) \\\\\n",
    "&= \\text{tr}(((XW - Y)W^T)^T dX) + \\text{tr}((X^T(XW - Y))^T dW)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Gradients:\n",
    "$$\n",
    "\\nabla_W f = X^T(XW - Y), \\qquad \\nabla_X f = (XW - Y)W^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee91a1",
   "metadata": {},
   "source": [
    "**Softmax + CE (row-wise)**\n",
    "\n",
    "For a row of logits (raw scores) $z \\in \\mathbb{R}^k$, define the softmax:\n",
    "$$\n",
    "s = \\text{softmax}(z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "s_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Given a target distribution $y \\in \\mathbb{R}^k$ (often one-hot) or label, the\n",
    "cross-entropy loss is\n",
    "$$\n",
    "\\ell(z, y) = -\\sum_i y_i \\log s_i = -y^T \\log s\n",
    "$$\n",
    "\n",
    "We know that\n",
    "\n",
    "$$\n",
    "\\log s_i = z_i - \\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "Differential:\n",
    "$$\n",
    "\\ell(z, y) \n",
    "= -y^Tz + (\\sum_i y_i) (\\log \\sum_j e^{z_j})\n",
    "= -y^Tz + \\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "d\\ell = -y + \\frac{\\mathbf{1}}{\\sum_j e^{z_j}} \\cdot (e^{z_1}, \\dotsb, e^{z_k})^T = -y + s\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "$$\n",
    "\\nabla_z \\ell = s - y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884e7de",
   "metadata": {},
   "source": [
    "**Elementwise activations**\n",
    "\n",
    "Let $\\phi$ be a scalar activation function (like ReLU, sigmoid, tanh, etc.) that is applied to each element of matrix $X \\in \\mathbb{R}^{m \\times n}$ independently.\n",
    "\n",
    "$$[\\phi(X)]_{ij} = \\phi(X_{ij})$$\n",
    "\n",
    "Suppose the loss $L$ depends on $\\phi(X)$, and you already know the upstream gradient\n",
    "$$\n",
    "G = \\nabla_{\\phi(X)} L \\in \\mathbb{R}^{m \\times n}\n",
    "$$\n",
    "\n",
    "Then by the chain rule (elementwise), the gradient of $L$ with respect to $X$ is:\n",
    "$$\n",
    "\\nabla_X L = G \\odot \\phi'(X)\n",
    "$$\n",
    "where $\\odot$ denotes elementwise multiplication.\n",
    "\n",
    "Special case: if the loss is a sum over all entries of $\\phi(X)$, i.e.\n",
    "$$\n",
    "L = \\sum_{ij} \\phi(X_{ij})\n",
    "$$\n",
    "\n",
    "then $G = \\mathbf{1}$ and:\n",
    "$$\n",
    "\\nabla_X L = \\phi'(X)\n",
    "$$\n",
    "\n",
    "Common activations and their derivatives:\n",
    "- ReLU:\n",
    "\n",
    "$$\\phi(x) = \\max(0, x), \\qquad \\phi'(x) = \\mathbf{1}[x > 0]$$\n",
    "\n",
    "- LeakyReLU ($\\alpha$):\n",
    "\n",
    "$$\\phi(x) = \\begin{cases} x & x > 0 \\\\ \\alpha x & x \\le 0 \\end{cases}, \\qquad\n",
    "\\phi'(x) = \\begin{cases} 1 & x > 0 \\\\ \\alpha & x \\le 0 \\end{cases}$$\n",
    "\n",
    "- Sigmoid $\\sigma(x)$:\n",
    "\n",
    "$$\\phi(x) = \\frac{1}{1 + e^{-x}}, \\qquad \\phi'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "- $\\tanh$:\n",
    "\n",
    "$$\\phi(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}, \\qquad \\phi'(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "- Softplus:\n",
    "\n",
    "$$\\phi(x) = \\log(1 + e^x), \\qquad \\phi'(x) = \\sigma(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a607c92",
   "metadata": {},
   "source": [
    "**Chain rules**\n",
    "\n",
    "Suppose we have a scalar loss $L$ that depends on an intermediate variable $Z$,\n",
    "which itself depends on $Y$, which in turn depends on $X$:\n",
    "\n",
    "$$\n",
    "X \\xrightarrow{g} Y \\xrightarrow{f} Z \\xrightarrow{} L\n",
    "$$\n",
    "\n",
    "We can always write the differential of the loss with respect to $Z$ as\n",
    "\n",
    "$$\n",
    "dL = \\text{tr}(G_Z^T dZ)\n",
    "$$\n",
    "\n",
    "where $G_Z = \\frac{\\partial L}{\\partial Z}$ is the upstream gradient arriving at $Z$.\n",
    "\n",
    "If $Z = f(Y)$ and $Y = g(X)$, then by the multivariate chain rule:\n",
    "\n",
    "$$\n",
    "G_X = J_{Y \\to X}^T G_Z\n",
    "$$\n",
    "\n",
    "where $J_{Y \\to X} = \\frac{\\partial Y}{\\partial X}$ is the Jacobian of $Y$ with respect to $X$.\n",
    "This form emphasizes that the **Jacobian is transposed** when propagating gradients backwards.\n",
    "\n",
    "In practice, the full Jacobian $J$ is rarely fully formed (which can be huge).\n",
    "Instead, we compute the **vector-Jacobian product (VJP)** directly:\n",
    "\n",
    "$$\n",
    "G_Y = J^T G_Z\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $J$ is the Jacobian of the local function $Z = f(Y)$\n",
    "- $G_Z$ is the upstream gradient $\\frac{\\partial L}{\\partial Z}$\n",
    "- $G_Y$ is the downstream gradient $\\frac{\\partial L}{\\partial Y}$, the result of the VJP\n",
    "\n",
    "This is exactly what reverse-mode autodiff (backpropagation) implements:\n",
    "it efficiently pushes $G$ backwards layer by layer without explicitly building the Jacobian.\n",
    "\n",
    "- Forward mode: propagate differentials $dZ = J \\, dX$\n",
    "- Reverse mode: propagate gradients $G_X = J^T \\, G_Z$\n",
    "- Backprop = chaining many VJPs over a neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
