{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e2f755",
   "metadata": {},
   "source": [
    "# Backpropagation via Matrix Calculus\n",
    "\n",
    "We will provide written matrix calculus derivations and the implementation for a reverse-mode autodiff engine, then use it to implement and verify backpropagation across common layers and losses:\n",
    "- Use matrix calculus (differentials, trace trick, chain rules) to derive gradients for:\n",
    "  - affine $\\rightarrow$ activation $\\rightarrow$ softmax-CE\n",
    "  - MSE regression\n",
    "  - L2 regularization\n",
    "- Implement a reverse-mode autodiff core (vector-Jacobian products) to reuse across layers\n",
    "- Verify correctness with a single finite-difference check per op\n",
    "- Train a simple 2-layer MLP on toy data (e.g. 2-moons) with SGD to demonstrate gradients flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88af6f4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890bc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc53254",
   "metadata": {},
   "source": [
    "## Matrix Calculus Basics\n",
    "\n",
    "### Intro\n",
    "- $\\Delta$: big, finite step (difference)\n",
    "- $d$: tiny, infinitesimal step (differential)\n",
    "- $\\partial$: derivative with respect to one variable (partial)\n",
    "- $\\nabla$: gradient operator (vector of partials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8180f2",
   "metadata": {},
   "source": [
    "### Differentials\n",
    "\n",
    "$d(\\cdot)$ means a differential, i.e. the infinitesimal change in a function when its inputs change a little. For a scalar function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$,\n",
    "\n",
    "$$df(x) = \\nabla f(x)^T dx \\approx f(x + \\Delta x) - f(x)$$\n",
    "\n",
    "where:\n",
    "- $dx$ is the vector of infinitesimal input changes\n",
    "- $\\nabla f(x)$ is the gradient (rate of change of $f$ with respect to each coordinate)\n",
    "\n",
    "So $d f(x)$ is the infinitesimal change in the output, predicted linearly from the gradient and input change.\n",
    "\n",
    "Let's consider the scalar function $f(x, y) = x^T Ay$. Let's see how $f$ changes when $x \\mapsto x + dx, y \\mapsto y + dy$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "d f(x, y) &= d(x^T Ay) \\\\\n",
    "&= (x + dx)^T A (y + dy) - x^T Ay \\\\\n",
    "&= dx^T Ay + x^T Ady + dx^T Ady \\\\\n",
    "&= (Ay)^T dx + (A^T x)^T dy\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We drop $dx^T Ady$ because it is the product of two infinitesimals.\n",
    "\n",
    "So the syntax $d(x^T Ay) = (Ay)^T dx + (A^T x)^T dy$ is a compact way of saying:\n",
    "- the gradient wrt $x$ is $Ay$\n",
    "- the gradient wrt $y$ is $A^T x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb7ea4",
   "metadata": {},
   "source": [
    "### Trace trick\n",
    "\n",
    "The trace of a square matrix is the sum of the diagonal elements.\n",
    "\n",
    "$$\\text{tr}(M) = \\sum_i M_{ii}$$\n",
    "\n",
    "The trace has a cyclic property meaning $\\text{tr}(ABC) = \\text{tr}(BCA) = \\text{tr}(CAB)$ and linearity meaning $\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)$.\n",
    "\n",
    "Gradients are sometimes easiest to express in terms of traces. The differential of a scalar function $f(X)$ with matrix input is often written:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "df &= \\sum_{i, j} \\frac{\\partial f}{\\partial X_{ij}} dX_{ij} \\\\\n",
    "&= \\text{tr}((\\frac{\\partial f}{\\partial X})^T dX) \\\\\n",
    "&= \\text{tr}(G^T dX)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $G$ is the gradient $\\frac{\\partial f}{\\partial X}$.\n",
    "\n",
    "For example, if we take the scalar function $f(X) = tr(A^T X)$, then its differential is $df = \\text{tr}(A^T dX)$ and the gradient matrix with respect to $X$ is $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebef283",
   "metadata": {},
   "source": [
    "### Example: Mini 2x2 Trace Trick\n",
    "\n",
    "Let \n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} \\\\\n",
    "x_{21} & x_{22}\n",
    "\\end{bmatrix}, \\qquad\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}, \\qquad\n",
    "f(X) = \\text{tr}(A^T X)\n",
    "$$\n",
    "\n",
    "If we expand $f(X)$, we see that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(X) &= \n",
    "\\text{tr}\\left(\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} \\\\\n",
    "x_{21} & x_{22}\n",
    "\\end{bmatrix}\n",
    "\\right) \\\\\n",
    "&= 1 \\cdot x_{11} + 2 \\cdot x_{12} + 3 \\cdot x_{21} + 4 \\cdot x_{22}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can compute the gradient by inspection\n",
    "\n",
    "$$\n",
    "\\nabla_X f =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix} = A\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a607c92",
   "metadata": {},
   "source": [
    "### Chain rules\n",
    "\n",
    "Suppose we have a scalar loss $L$ that depends on an intermediate variable $Z$,\n",
    "which itself depends on $Y$, which in turn depends on $X$:\n",
    "\n",
    "$$\n",
    "X \\xrightarrow{g} Y \\xrightarrow{f} Z \\xrightarrow{} L\n",
    "$$\n",
    "\n",
    "We can always write the differential of the loss with respect to $Z$ as\n",
    "\n",
    "$$\n",
    "dL = \\text{tr}(G_Z^T dZ)\n",
    "$$\n",
    "\n",
    "where $G_Z = \\frac{\\partial L}{\\partial Z}$ is the upstream gradient arriving at $Z$.\n",
    "\n",
    "If $Z = f(Y)$ and $Y = g(X)$, then by the multivariate chain rule:\n",
    "\n",
    "$$\n",
    "G_X = J_{Y \\to X}^T J_{Z \\to Y}^T G_Z = J_{Z \\to X}^T G_Z\n",
    "$$\n",
    "\n",
    "where $J_{Z \\to X} = \\frac{\\partial Z}{\\partial X}$ is the Jacobian of $Z$ with respect to $X$.\n",
    "This form emphasizes that the Jacobian is transposed when propagating gradients backwards.\n",
    "\n",
    "In practice, the full Jacobian $J$ is rarely fully formed (which can be huge).\n",
    "Instead, we compute the **vector-Jacobian product (VJP)** using structured rules (e.g. elementwise multiply for ReLU, matrix multiply for linear layers, broadcasting for sums/means):\n",
    "\n",
    "$$\n",
    "G_Y = J^T G_Z\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $J$ is the Jacobian of the local function $Z = f(Y)$\n",
    "- $G_Z$ is the upstream gradient $\\frac{\\partial L}{\\partial Z}$\n",
    "- $G_Y$ is the downstream gradient $\\frac{\\partial L}{\\partial Y}$, the result of the VJP\n",
    "\n",
    "This is exactly what reverse-mode autodiff (backpropagation) implements:\n",
    "it efficiently pushes $G$ backwards layer by layer without explicitly building the Jacobian.\n",
    "\n",
    "- Forward mode: propagate differentials $dZ = J \\, dX$\n",
    "- Reverse mode: propagate gradients $G_X = J^T \\, G_Z$\n",
    "- Backprop = chaining many VJPs over a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ba1e2",
   "metadata": {},
   "source": [
    "### Example: Mini 2x2 VJP\n",
    "\n",
    "Let have a simple mapping $Z = f(Y)$ where both $Y, Z \\in \\mathbb{R}^2$.\n",
    "\n",
    "$$\n",
    "f\\left(\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}\\right) = \\begin{bmatrix} y_1 + 2y_2 \\\\ 3y_1 - y_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Jacobian is\n",
    "\n",
    "$$\n",
    "J = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial z_1}{\\partial y_1} & \\frac{\\partial z_1}{\\partial y_2} \\\\[8pt]\n",
    "\\frac{\\partial z_2}{\\partial y_1} & \\frac{\\partial z_2}{\\partial y_2}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Suppose the upstream gradient is\n",
    "\n",
    "$$\n",
    "G_Z = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The VJP and $G_Y$, the downstream gradient wrt $Y$, is\n",
    "\n",
    "$$\n",
    "G_Y = J^T G_Z = \n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & -1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "-2\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "-5 \\\\\n",
    "4\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d1ae7",
   "metadata": {},
   "source": [
    "### Broadcasting rules\n",
    "\n",
    "When a smaller tensor is broadcasted to match a larger shape, the forward op duplicates values. In backprop, the gradient must be summed over the broadcasted dimensions to undo the duplication.\n",
    "\n",
    "**Example (add bias):**\n",
    "$$\n",
    "Y = X + b, \\quad X \\in \\mathbb{R}^{m \\times n}, \\quad b \\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "By definition\n",
    "\n",
    "$$\n",
    "dY_{ij} = dX_{ij} + db_j\n",
    "$$\n",
    "\n",
    "For a scalar loss $L(Y)$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dL &= \\sum_{i,j} \\frac{\\partial L}{\\partial Y_{ij}} dY_{ij} \\\\\n",
    "&= \\sum_{i,j} \\frac{\\partial L}{\\partial Y_{ij}} (dX_{ij} + db_j) \\\\\n",
    "&= \\sum_{i,j} \\frac{\\partial L}{\\partial Y_{ij}} dX_{ij} + \\sum_{i,j} \\frac{\\partial L}{\\partial Y_{ij}} db_j \\\\\n",
    "&= \\sum_{i,j} \\frac{\\partial L}{\\partial Y_{ij}} dX_{ij} + \\sum_{j} \\left(\\sum_{i} \\frac{\\partial L}{\\partial Y_{ij}}\\right) db_j\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By definition of differentials for a scalar $L$ as a function of $(X, b)$\n",
    "\n",
    "$$\n",
    "dL = \\sum_{i,j} \\frac{\\partial L}{\\partial X_{ij}} dX_{ij} + \\sum_j \\frac{\\partial L}{\\partial b_j} db_j\n",
    "$$\n",
    "\n",
    "So from the first coefficients, we see that $\\nabla_X L = \\nabla_Y L$. From the second term, we see that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{ij}} = \\frac{\\partial L}{\\partial Y_{ij}} \\implies \\nabla_X L = \\nabla_Y L\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_j} = \\sum_i \\frac{\\partial L}{\\partial Y_{ij}} \\implies \\nabla_b L = \\mathbf{1}^T \\nabla_Y L\n",
    "$$\n",
    "\n",
    "where $\\mathbf{1} \\in \\mathbb{R}^m$.\n",
    "\n",
    "Forward: $b$ is broadcast to shape $(m, n)$\n",
    "\n",
    "$$\n",
    "\\tilde{B} =\n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & \\cdots & b_n \\\\\n",
    "b_1 & b_2 & \\cdots & b_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_1 & b_2 & \\cdots & b_n\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{m \\times n}\n",
    "$$\n",
    "\n",
    "Backward:\n",
    "- $\\nabla_X L = \\nabla_Y L$\n",
    "- $(\\nabla_b L)_j = \\sum_{i=1}^m \\frac{\\partial L}{\\partial Y_{ij}}$\n",
    "\n",
    "So the gradient wrt $b$ is the column-wise sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce6eb35",
   "metadata": {},
   "source": [
    "### Example: Mini 3x2 Broadcasting\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} \\\\\n",
    "x_{21} & x_{22} \\\\\n",
    "x_{31} & x_{32}\n",
    "\\end{bmatrix}, \\quad\n",
    "b = \\begin{bmatrix} b_1 & b_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The forward op is $Y = X + b$ where $b$ is broadcast to shape $3 \\times 2$:\n",
    "\n",
    "$$\n",
    "\\tilde{B} =\n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 \\\\\n",
    "b_1 & b_2 \\\\\n",
    "b_1 & b_2\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "Y = X + \\tilde{B}\n",
    "$$\n",
    "\n",
    "So explicitly\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "x_{11} + b_1 & x_{12} + b_2 \\\\\n",
    "x_{21} + b_1 & x_{22} + b_2 \\\\\n",
    "x_{31} + b_1 & x_{32} + b_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Suppose the upstream gradient is\n",
    "\n",
    "$$\n",
    "\\nabla_Y L =\n",
    "\\begin{bmatrix}\n",
    "g_{11} & g_{12} \\\\\n",
    "g_{21} & g_{22} \\\\\n",
    "g_{31} & g_{32}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The gradient wrt $X$ and $b$ respectively are\n",
    "\n",
    "$$\n",
    "\\nabla_X L = \\nabla_Y L\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b L =\n",
    "\\begin{bmatrix}\n",
    "g_{11} + g_{21} + g_{31} \\\\[6pt]\n",
    "g_{12} + g_{22} + g_{32}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005172a",
   "metadata": {},
   "source": [
    "### Reduction rules\n",
    "\n",
    "When an operation reduces a tensor along some axis (e.g. sum, mean), the forward op collapses values. In backprop, the gradient must be broadcast back to the original shape to undo the reduction.\n",
    "\n",
    "**Example (sum reduction):**\n",
    "\n",
    "$$\n",
    "y = \\sum_{i} x_i, \\quad x \\in \\mathbb{R}^m, \\quad y \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "By definition\n",
    "\n",
    "$$\n",
    "dy = \\sum_{i} dx_i\n",
    "$$\n",
    "\n",
    "For a scalar loss $L(y)$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dL &= \\frac{\\partial L}{\\partial y} \\, dy \\\\\n",
    "&= \\frac{\\partial L}{\\partial y} \\left(\\sum_{i} dx_i\\right) \\\\\n",
    "&= \\sum_{i} \\left(\\frac{\\partial L}{\\partial y}\\right) dx_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By definition of differentials for a scalar $L$ as a function of $x$\n",
    "\n",
    "$$\n",
    "dL = \\sum_{i} \\frac{\\partial L}{\\partial x_i} dx_i\n",
    "$$\n",
    "\n",
    "Comparing coefficients, we see\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial y}\n",
    "$$\n",
    "\n",
    "So the gradient is broadcast:\n",
    "\n",
    "$$\n",
    "\\nabla_x L = \\mathbf{1} \\nabla_y L\n",
    "$$\n",
    "\n",
    "where $\\mathbf{1} \\in \\mathbb{R}^m$ is a vector of ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c64c7f",
   "metadata": {},
   "source": [
    "### Example: Mini 3x2 Reduction\n",
    "\n",
    "Let $y$ be the column-wise sum\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & -2 \\\\\n",
    "0 & 3 \\\\\n",
    "4 & 1\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3\\times 2}, \\qquad\n",
    "y = \n",
    "\\begin{bmatrix}\n",
    "1+0+4 \\\\\n",
    "-2+3+1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "5 \\\\ 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Suppose the upstream gradient is\n",
    "$$\n",
    "\\nabla_y L =\n",
    "\\begin{bmatrix}\n",
    "2 \\\\ -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The gradient wrt $X$ is\n",
    "$$\n",
    "\\nabla_X L =\n",
    "\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "2 & -1 \\\\\n",
    "2 & -1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fdd9d",
   "metadata": {},
   "source": [
    "## Layer and Loss Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9b8e9",
   "metadata": {},
   "source": [
    "### Quadratic forms\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{N \\times d}$, $W \\in \\mathbb{R}^{d \\times k}$, and $Y \\in \\mathbb{R}^{N \\times k}$. Define the loss:\n",
    "\n",
    "$$f(X, W) = \\frac{1}{2} \\|XW - Y\\|_F^2 = \\frac{1}{2}\\text{tr}((XW - Y)^T (XW - Y))$$\n",
    "\n",
    "Differential:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "df &= \\text{tr}((XW - Y)^T (dX \\, W + X \\, dW)) \\\\\n",
    "&= \\text{tr}(((XW - Y)W^T)^T dX) + \\text{tr}((X^T(XW - Y))^T dW)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Gradients:\n",
    "$$\n",
    "\\nabla_W f = X^T(XW - Y), \\qquad \\nabla_X f = (XW - Y)W^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee91a1",
   "metadata": {},
   "source": [
    "### Softmax + CE (row-wise)\n",
    "\n",
    "For a row of logits (raw scores) $z \\in \\mathbb{R}^k$, define the softmax:\n",
    "$$\n",
    "s = \\text{softmax}(z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "s_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Given a target distribution $y \\in \\mathbb{R}^k$ (often one-hot) or label, the\n",
    "cross-entropy loss is\n",
    "$$\n",
    "\\ell(z, y) = -\\sum_i y_i \\log s_i = -y^T \\log s\n",
    "$$\n",
    "\n",
    "We know that\n",
    "\n",
    "$$\n",
    "\\log s_i = z_i - \\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "Differential:\n",
    "$$\n",
    "\\ell(z, y) \n",
    "= -y^Tz + (\\sum_i y_i) (\\log \\sum_j e^{z_j})\n",
    "= -y^Tz + \\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "d\\ell = -y + \\frac{\\mathbf{1}}{\\sum_j e^{z_j}} \\cdot (e^{z_1}, \\dotsb, e^{z_k})^T = -y + s\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "$$\n",
    "\\nabla_z \\ell = s - y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a4f42",
   "metadata": {},
   "source": [
    "### Stable form softmax + CE (log-sum-exp trick)\n",
    "\n",
    "Directly computing $\\log \\sum_j e^{z_j}$ can overflow if some $z_j$ are large. To fix this, subtract the max logit $m = \\max_j z_j$ before exponentiating:\n",
    "\n",
    "$$\n",
    "\\log \\sum_j e^{z_j} = m + \\log \\sum_j e^{z_j - m}\n",
    "$$\n",
    "\n",
    "This keeps all exponentials in $[0, 1]$ and is numerically stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884e7de",
   "metadata": {},
   "source": [
    "### Elementwise activations\n",
    "\n",
    "Let $\\phi$ be a scalar activation function (like ReLU, sigmoid, tanh, etc.) that is applied to each element of matrix $X \\in \\mathbb{R}^{m \\times n}$ independently.\n",
    "\n",
    "$$[\\phi(X)]_{ij} = \\phi(X_{ij})$$\n",
    "\n",
    "Suppose the loss $L$ depends on $\\phi(X)$, and you already know the upstream gradient\n",
    "$$\n",
    "G = \\nabla_{\\phi(X)} L \\in \\mathbb{R}^{m \\times n}\n",
    "$$\n",
    "\n",
    "Then by the chain rule (elementwise), the gradient of $L$ with respect to $X$ is:\n",
    "$$\n",
    "\\nabla_X L = G \\odot \\phi'(X)\n",
    "$$\n",
    "where $\\odot$ denotes elementwise multiplication.\n",
    "\n",
    "Special case: if the loss is a sum over all entries of $\\phi(X)$, i.e.\n",
    "$$\n",
    "L = \\sum_{ij} \\phi(X_{ij})\n",
    "$$\n",
    "\n",
    "then $G = \\mathbf{1}$ and:\n",
    "$$\n",
    "\\nabla_X L = \\phi'(X)\n",
    "$$\n",
    "\n",
    "Common activations and their derivatives:\n",
    "- ReLU:\n",
    "\n",
    "$$\\phi(x) = \\max(0, x), \\qquad \\phi'(x) = \\mathbf{1}[x > 0]$$\n",
    "\n",
    "- LeakyReLU ($\\alpha$):\n",
    "\n",
    "$$\\phi(x) = \\begin{cases} x & x > 0 \\\\ \\alpha x & x \\le 0 \\end{cases}, \\qquad\n",
    "\\phi'(x) = \\begin{cases} 1 & x > 0 \\\\ \\alpha & x \\le 0 \\end{cases}$$\n",
    "\n",
    "- Sigmoid $\\sigma(x)$:\n",
    "\n",
    "$$\\phi(x) = \\frac{1}{1 + e^{-x}}, \\qquad \\phi'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "- $\\tanh$:\n",
    "\n",
    "$$\\phi(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}, \\qquad \\phi'(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "- Softplus:\n",
    "\n",
    "$$\\phi(x) = \\log(1 + e^x), \\qquad \\phi'(x) = \\sigma(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8c6e0",
   "metadata": {},
   "source": [
    "### Affine layer\n",
    "\n",
    "An affine layer is a lienar transformation of the input with weights and a bias, a basic \"fully-connected\" layer tha maps inputs into a new feature space before applying nonlinearities.\n",
    "\n",
    "$$\n",
    "Z = XW + \\mathbf{1} b^T\n",
    "$$\n",
    "\n",
    "where:\n",
    "- inputs $X \\in \\mathbb{R}^{N \\times d}$ (each row is a sample)\n",
    "- weights $W \\in \\mathbb{R}^{d \\times k}$\n",
    "- bias $b \\in \\mathbb{R}^k$ (broadcast to each row, $1 \\in \\mathbb{R}^N$ is a column of 1s)\n",
    "- outputs $Z \\in \\mathbb{R}^{N \\times k}$\n",
    "\n",
    "Let's assume that the upstream gradient is $G = \\nabla_Z L \\in \\mathbb{R}^{N \\times k}$\n",
    "\n",
    "Differential:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dZ &= dX W + X dW + \\mathbf{1} (db)^T \\\\\n",
    "dL &= \\text{tr}(G^T dZ) \\\\\n",
    "&= \\text{tr}(G^T dX W) + \\text{tr}(G^T X dW) + \\text{tr}(G^T \\mathbf{1} (db)^T) \\\\\n",
    "&= \\text{tr}((GW^T)^T dX) + \\text{tr}((X^T G)^T dW) + (G^T \\mathbf{1})^T db\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Gradients:\n",
    "\n",
    "$$\n",
    "\\nabla_X L = G W^T \\in \\mathbb{R}^{N \\times d}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\qquad \\nabla_W L = X^T G \\in \\mathbb{R}^{d \\times k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b L = G^T \\mathbf{1} = \\sum_{n=1}^N G_n \\in \\mathbb{R}^{k}\n",
    "$$\n",
    "\n",
    "If you augment inputs with a column of ones $\\tilde{X} = \\begin{bmatrix} X & \\mathbf{1} \\end{bmatrix}$ and $\\tilde{W} = \\begin{bmatrix} W \\\\ b^T \\end{bmatrix}$, then $Z = \\tilde{X} \\tilde{W}$ and $\\nabla_{\\tilde{W}} L = \\tilde{X}^T G$ and the last row recovers $\\nabla_b L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1631e",
   "metadata": {},
   "source": [
    "### L2 weight decay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
