{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e2f755",
   "metadata": {},
   "source": [
    "# Backpropagation via Matrix Calculus\n",
    "\n",
    "We will provide matrix-calculus derivations to implementing a reverse-mode autodiff engine, then use it to implement and verify backpropagation across common layers and losses:\n",
    "- Derive vector and matrix form gradients using differentials, the trace trick, and chain rules\n",
    "- Implement a reverse-mode autodiff core (vector-Jacobian products)\n",
    "- Derive and code backprop for: affine $\\rightarrow$ activation $\\rightarrow$ softmax-CE; MSE regression; L2 regularization; BatchNorm (stretch)\n",
    "- Verify gradients with high-precision finite differences on randomized cases\n",
    "- Show performance wins of reverse vs forward mode on wide nets (empirical scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88af6f4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890bc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c635fa",
   "metadata": {},
   "source": [
    "## Matrix Calculus Basics\n",
    "\n",
    "**Intro**\n",
    "- $\\Delta$: big, finite step (difference)\n",
    "- $d$: tiny, infinitesimal step (differential)\n",
    "- $\\nabla$: operator that gives you the gradient (vector of partials)\n",
    "\n",
    "**Differentials**\n",
    "\n",
    "$d(\\cdot)$ means a differential, i.e. the infinitesimal change in a function when its inputs change a little. For a scalar function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$,\n",
    "\n",
    "$$df(x) = \\nabla f(x)^T dx \\approx f(x + \\Delta x) - f(x)$$\n",
    "\n",
    "where:\n",
    "- $dx$ is the vector of infinitesimal input changes\n",
    "- $\\nabla f(x)$ is the gradient (rate of change of $f$ with respect to each coordinate)\n",
    "\n",
    "So $d f(x)$ is the infinitesimal change in the output, predicted linearly from the gradient and input change.\n",
    "\n",
    "Let's consider the scalar function $f(x, y) = x^T Ay$. Let's see how $f$ changes when $x \\mapsto x + dx, y \\mapsto y + dy$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "d f(x, y) &= d(x^T Ay) \\\\\n",
    "&= (x + dx)^T A (y + dy) - x^T Ay \\\\\n",
    "&= dx^T Ay + x^T Ady + dx^T Ady \\\\\n",
    "&= (Ay)^T dx + (A^T x)^T dy\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We drop $dx^T Ady$ because it is the product of two infinitesimals.\n",
    "\n",
    "So the syntax $d(x^T Ay) = (Ay)^T dx + (A^T x)^T dy$ is a compact way of saying:\n",
    "- the gradient wrt $x$ is $Ay$\n",
    "- the gradient wrt $y$ is $A^T x$\n",
    "\n",
    "**Trace trick**\n",
    "\n",
    "The trace of a square matrix is the sum of the diagonal elements.\n",
    "\n",
    "$$\\text{tr}(M) = \\Sigma_i M_{ii}$$\n",
    "\n",
    "The trace has a cyclic property meaning $\\text{tr}(ABC) = \\text{tr}(BCA) = \\text{tr}(CAB)$ and linearity meaning $\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)$.\n",
    "\n",
    "Gradients are sometimes easiest to express in terms of traces. The differential of a scalar function $f(X)$ with matrix input is often written:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "df &= \\Sigma_{i, j} \\frac{\\partial f}{\\partial X_{ij}} dX_{ij} \\\\\n",
    "&= \\text{tr}((\\frac{\\partial f}{\\partial X})^T dX) \\\\\n",
    "&= \\text{tr}(G^T dX)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $G$ is the gradient $\\frac{\\partial f}{\\partial X}$.\n",
    "\n",
    "For example, if we take the scalar function $f(X) = tr(A^T X)$, then its differential is $df = \\text{tr}(A^T dX)$ and the gradient matrix with respect to $X$ is $A$.\n",
    "\n",
    "**Quadratic forms**\n",
    "- $f(X) = \\frac{1}{2}||XW - Y||^2_F \\implies \\nabla_W f = X^T (XW - Y), \\nabla_X f = (XW - Y)W^T$\n",
    "\n",
    "**Softmax + CE (row-wise)**\n",
    "- $softmax(z)_i = \\frac{e^{z_i}}{\\Sigma_j e^{z_j}}, \\ell = -\\Sigma_i y_i \\log s_i \\implies \\nabla_z \\ell = s - y$\n",
    "\n",
    "**Elementwise activations**\n",
    "- $\\nabla_X\\Sigma \\phi (X) = \\phi' (X)$\n",
    "\n",
    "**Chain rules**\n",
    "- Matrix chain: $dL = \\text{tr}(G^T dZ), Z = f(Y), Y = g(X) \\implies G_X = J^T_{Y \\rightarrow X} G_Z$\n",
    "- VJP: given upstream $G$, compute $G \\cdot J$ without forming $J$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
