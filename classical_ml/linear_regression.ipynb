{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b26d625",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "This notebook builds a simple linear regression model.\n",
    "- Derives the MLE solution for estimating weights under Gaussian noise\n",
    "- Verifies the closed-form least squares solution using gradient descent\n",
    "- Implements Bayesian inference with Gaussian priors\n",
    "- Visualizes predictive uncertainty from the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be974ab",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa38830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0ac6d",
   "metadata": {},
   "source": [
    "## MLE for Linear Regression with Gaussian Noise\n",
    "\n",
    "We're modeling outputs of $y \\in \\mathbb{R}^n$ as noisy linear combinations of inputs $X \\in \\mathbb{R}^{n \\times d}$:\n",
    "\n",
    "$$y = X w + \\epsilon$$\n",
    "\n",
    "Where: \n",
    "- $x_i \\in \\mathbb{R}^d$ is the $i$-th row of $X$, representing one data point\n",
    "- $w \\in \\mathbb{R}^d$ is the parameter vector\n",
    "- Each output $y_i$ is given by $y_i = x_i^T w + \\epsilon_i$\n",
    "- The noise term $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ is i.i.d. gaussian\n",
    "\n",
    "Conditioned on $X$, the outputs are:\n",
    "\n",
    "$$y \\mid X \\sim \\mathbb{N}(X w, \\sigma^2 I)$$\n",
    "\n",
    "When $d = 1$, we are fitting a line and when $d = 2$, we are fitting a plane. Also, note that there is no bias term although that could be added by adjusting the model.\n",
    "\n",
    "$$X' = \\begin{bmatrix} X & \\mathbf{1} \\end{bmatrix}, w' = \\begin{bmatrix} w \\\\ b \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4173f",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "The probability density function of $y$ is the following.\n",
    "\n",
    "$$ p(y | X, w, \\sigma^2) = \\prod_{i=1}^{n} \\mathcal{N}(y_i | x_i^T w, \\sigma^2)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\mathcal{N}(y_i | x_i^T, w, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp (-\\frac{1}{2 \\sigma^2} (y_i - x_i^T w)^2)$$\n",
    "\n",
    "When we want the log-likelihood and remove the constants, we get the following.\n",
    "\n",
    "$$\\log(p(y | X, w, \\sigma^2)) \\sim -\\sum_{i=1}^{n} (y_i - x_i^T w)^2 \\sim -||y - Xw||^2$$\n",
    "\n",
    "This means that maximizing the log-likelihood is equivalent to minimizing the squared error.\n",
    "\n",
    "$$\\min_{w} ||y - Xw||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4088c3",
   "metadata": {},
   "source": [
    "### Closed-form Solution\n",
    "\n",
    "We can find the formula for the weights that minimize the least squared error by solving for the closed-form solution. \n",
    "\n",
    "$$L(w) = ||y - Xw||^2 = y^T y - 2 w^T X^T y + w^T X^T X w$$\n",
    "\n",
    "So, we take the derivative of the loss.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dw} L(w) &= -2 X^T y + 2 X^T X w \\\\\n",
    "&= -2 X^T (y - Xw)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can set the derivative for $0$ and solve for $w$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-2 X^T (y - Xw) &= 0 \\\\\n",
    "X^T (y - Xw) &= 0 \\\\\n",
    "X^T y &= X^T Xw \\\\\n",
    "w &= (X^T X)^{-1} X^T y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, the least squares solution is the following.\n",
    "\n",
    "$$w = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f7006",
   "metadata": {},
   "source": [
    "### Verifying using Gradient Descent\n",
    "\n",
    "We're trying to minimize the negative log-likelihood:\n",
    "\n",
    "$$L(w) = ||y - Xw||^2$$\n",
    "\n",
    "The gradient of the loss tells us the direction of steepest increase in loss, so moving in the opposite direction leads us toward lower loss.\n",
    "\n",
    "$$\\frac{d}{dw} L(w) = -2X^T(y - Xw)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8429dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3520653267642995)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gaussian_pdf(x, mu, sigma):\n",
    "    \"\"\"\n",
    "    Gaussian probability density function\n",
    "\n",
    "    Args:\n",
    "        x: point(s) to get the probability of generating from a normal distribution\n",
    "        mu: mean of the normal distribution\n",
    "        sigma: standard deviation of the normal distribution\n",
    "\n",
    "    Returns:\n",
    "        The probability density at x\n",
    "    \"\"\"\n",
    "    coef = 1 / (np.sqrt(2 * np.pi * sigma ** 2))\n",
    "    exponent = -((x - mu) ** 2 / (2 * sigma ** 2))\n",
    "    return coef * np.exp(exponent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
