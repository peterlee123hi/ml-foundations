{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b26d625",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "This notebook builds a simple linear regression model.\n",
    "- Derives the MLE solution for estimating weights under Gaussian noise\n",
    "- Verifies the closed-form least squares solution using gradient descent\n",
    "- Implements Bayesian inference with Gaussian priors\n",
    "- Visualizes predictive uncertainty from the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be974ab",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa38830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0ac6d",
   "metadata": {},
   "source": [
    "## MLE for Linear Regression with Gaussian Noise\n",
    "\n",
    "We're modeling outputs of $y \\in \\mathbb{R}^n$ as noisy linear combinations of inputs $X \\in \\mathbb{R}^{n \\times d}$:\n",
    "\n",
    "$$y = X w + \\epsilon$$\n",
    "\n",
    "Where: \n",
    "- $x_i \\in \\mathbb{R}^d$ is the $i$-th row of $X$, representing one data point\n",
    "- $w \\in \\mathbb{R}^d$ is the parameter vector\n",
    "- Each output $y_i$ is given by $y_i = x_i^T w + \\epsilon_i$\n",
    "- The noise term $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ is i.i.d. gaussian\n",
    "\n",
    "Conditioned on $X$, the outputs are:\n",
    "\n",
    "$$y \\mid X \\sim \\mathbb{N}(X w, \\sigma^2 I)$$\n",
    "\n",
    "When $d = 1$, we are fitting a line and when $d = 2$, we are fitting a plane. Also, note that there is no bias term although that could be added by adjusting the model.\n",
    "\n",
    "$$X' = \\begin{bmatrix} X & \\mathbf{1} \\end{bmatrix}, w' = \\begin{bmatrix} w \\\\ b \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4173f",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "The probability density function of $y$ is the following.\n",
    "\n",
    "$$ p(y | X, w, \\sigma^2) = \\prod_{i=1}^{n} \\mathcal{N}(y_i | x_i^T w, \\sigma^2)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\mathcal{N}(y_i | x_i^T, w, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp (-\\frac{1}{2 \\sigma^2} (y_i - x_i^T w)^2)$$\n",
    "\n",
    "When we want the log-likelihood and remove the constants, we get the following.\n",
    "\n",
    "$$\\log(p(y | X, w, \\sigma^2)) \\sim -\\sum_{i=1}^{n} (y_i - x_i^T w)^2 \\sim -||y - Xw||^2$$\n",
    "\n",
    "This means that maximizing the log-likelihood is equivalent to minimizing the squared error.\n",
    "\n",
    "$$\\min_{w} ||y - Xw||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4088c3",
   "metadata": {},
   "source": [
    "### Closed-form Solution\n",
    "\n",
    "We can find the formula for the weights that minimize the least squared error by solving for the closed-form solution. \n",
    "\n",
    "$$L(w) = ||y - Xw||^2 = y^T y - 2 w^T X^T y + w^T X^T X w$$\n",
    "\n",
    "So, we take the derivative of the loss.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dw} L(w) &= -2 X^T y + 2 X^T X w \\\\\n",
    "&= -2 X^T (y - Xw)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can set the derivative for $0$ and solve for $w$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-2 X^T (y - Xw) &= 0 \\\\\n",
    "X^T (y - Xw) &= 0 \\\\\n",
    "X^T y &= X^T Xw \\\\\n",
    "w &= (X^T X)^{-1} X^T y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, the least squares solution is the following.\n",
    "\n",
    "$$w = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d6ce647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_mle(X, y):\n",
    "    \"\"\"\n",
    "    Linear regression closed-form solution (MLE under Gaussian noise)\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target vector of shape (n_samples,)\n",
    "\n",
    "    Returns:\n",
    "        Weight vector w that minimizes squared error, of shape (n_features,)\n",
    "    \"\"\"\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f7006",
   "metadata": {},
   "source": [
    "### Verifying using Gradient Descent\n",
    "\n",
    "We're trying to minimize the negative log-likelihood:\n",
    "\n",
    "$$L(w) = ||y - Xw||^2$$\n",
    "\n",
    "The gradient of the loss tells us the direction of steepest increase in loss, so moving in the opposite direction leads us toward lower loss.\n",
    "\n",
    "$$\\frac{d}{dw} L(w) = -2X^T(y - Xw)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8429dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_true: [0.5 0.5]\n",
      "w_mle: [0.40740458 0.54641139]\n",
      "w_grad: [0.42134954 0.5317374 ]\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_gradient(w, X, y):\n",
    "    \"\"\"\n",
    "        Computes the gradient of the squared loss for linear regression\n",
    "\n",
    "        Args:\n",
    "            w: Current weight vector of shape (n_features,)\n",
    "            X: Feature matrix of shape (n_samples, n_features)\n",
    "            y: Target vector of shape (n_samples,)\n",
    "\n",
    "        Returns:\n",
    "            Gradient vector of shape (n_features,)\n",
    "    \"\"\"\n",
    "    return -2 * X.T @ (y - X @ w)\n",
    "\n",
    "# Fitting a plane\n",
    "n_features = 2\n",
    "\n",
    "# Generate true weights\n",
    "w_true = np.array([0.5, 0.5])  # np.random.rand(n_features)\n",
    "print('w_true:', w_true)\n",
    "\n",
    "# Generate random samples\n",
    "n_samples = 1000\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "\n",
    "# Generate noise (multiplying by sigma transforms N(0, 1) to N(0, sigma^2))\n",
    "sigma = 2\n",
    "epsilon = np.random.randn(n_samples) * sigma\n",
    "\n",
    "# Generate labels for samples\n",
    "y = X @ w_true + epsilon\n",
    "\n",
    "# Find estimated weights using closed-form MLE solution\n",
    "w_mle = linear_regression_mle(X, y)\n",
    "print('w_mle:', w_mle)\n",
    "\n",
    "# Verify closed-form solution using gradient descent\n",
    "w_grad = np.array([0.0, 0.0])\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.9\n",
    "for i in range(1000):\n",
    "    grad_vec = linear_regression_gradient(w_grad, X, y)\n",
    "    w_grad = w_grad - learning_rate * grad_vec\n",
    "    learning_rate *= learning_rate_decay\n",
    "print('w_grad:', w_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689e527",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression\n",
    "\n",
    "We're using Bayesian inference to:\n",
    "- Compute the posterior distribution over the weights given a prior distribution and likelihood\n",
    "- Sample from this posterior to visualize the predictive uncertainty\n",
    "- Show how confidence intervals shrink where data is dense and grow where data is sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe07ce5c",
   "metadata": {},
   "source": [
    "For Bayesian inference, instead of treating $w$ as a variable, we treat it as a random variable. Let's assume that the prior over the weights is Gaussian.\n",
    "\n",
    "$$w \\sim \\mathcal{N}(0, \\sigma_w^2 I) \\propto \\exp(-\\frac{1}{2\\sigma_w^2}||w||^2)$$\n",
    "\n",
    "As in the MLE exercise, we're modeling a linear model with Gaussian noise so the likelihood is Gaussian given $X$ and $w$.\n",
    "\n",
    "$$y | X, w \\sim \\mathcal{N}(Xw, \\sigma^2 I) \\propto \\exp(-\\frac{1}{2\\sigma^2}||y - Xw||^2)$$\n",
    "\n",
    "Now that we have prior distribution and likelihood, we can compute the posterior distribution using Bayes' Theorem.\n",
    "\n",
    "$$P(w | X, y) \\propto P(y | X, w) P(w)$$\n",
    "\n",
    "There is a lengthy derivation on how to the find resulting mean $\\mu_{post}$ and covariance $\\Sigma_{post}$ that starts with the following.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(w | X, y) &\\propto \\log p(y | X, w) + \\log p(w) \\\\\n",
    "&= -\\frac{1}{2\\sigma^2}(y - Xw)^T(y - Xw) - \\frac{1}{2\\sigma_w^2}w^T w + \\text{const}\n",
    "\\end{align}$$\n",
    "\n",
    "Eventually, we find that the posterior distribution is the following.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w | X, y &\\sim \\mathcal{N}(\\mu_{post}, \\Sigma_{post}) \\\\\n",
    "\\Sigma_{post} &= (\\frac{1}{\\sigma^2} X^T X + \\frac{1}{\\sigma_w^2} I)^{-1} \\\\\n",
    "\\mu_{post} &= \\Sigma_{post}(\\frac{1}{\\sigma^2}X^T y)\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
