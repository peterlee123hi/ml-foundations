{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b26d625",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "This notebook builds a simple linear regression model.\n",
    "- Derives the MLE solution for estimating weights under Gaussian noise\n",
    "- Verifies the closed-form least squares solution using gradient descent\n",
    "- Implements Bayesian inference with Gaussian priors\n",
    "- Visualizes predictive uncertainty from the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be974ab",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa38830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0ac6d",
   "metadata": {},
   "source": [
    "## MLE for Linear Regression with Gaussian Noise\n",
    "\n",
    "We're modeling outputs of $y \\in \\mathbb{R}^n$ as noisy linear combinations of inputs $X \\in \\mathbb{R}^{n \\times d}$:\n",
    "\n",
    "$$y = X w + \\epsilon$$\n",
    "\n",
    "Where: \n",
    "- $x_i \\in \\mathbb{R}^d$ is the $i$-th row of $X$, representing one data point\n",
    "- $w \\in \\mathbb{R}^d$ is the parameter vector\n",
    "- Each output $y_i$ is given by $y_i = x_i^T w + \\epsilon_i$\n",
    "- The noise term $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ is i.i.d. gaussian\n",
    "\n",
    "Conditioned on $X$, the outputs are:\n",
    "\n",
    "$$y \\mid X \\sim \\mathbb{N}(X w, \\sigma^2 I)$$\n",
    "\n",
    "When $d = 1$, we are fitting a line and when $d = 2$, we are fitting a plane. Also, note that there is no bias term although that could be added by adjusting the model.\n",
    "\n",
    "$$X' = \\begin{bmatrix} X & \\mathbf{1} \\end{bmatrix}, w' = \\begin{bmatrix} w \\\\ b \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4173f",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "The probability density function of $y$ is the following.\n",
    "\n",
    "$$ p(y | X, w, \\sigma^2) = \\prod_{i=1}^{n} \\mathcal{N}(y_i | x_i^T w, \\sigma^2)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\mathcal{N}(y_i | x_i^T, w, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp (-\\frac{1}{2 \\sigma^2} (y_i - x_i^T w)^2)$$\n",
    "\n",
    "When we want the log-likelihood and remove the constants, we get the following.\n",
    "\n",
    "$$\\log(p(y | X, w, \\sigma^2)) \\sim -\\sum_{i=1}^{n} (y_i - x_i^T w)^2 \\sim -||y - Xw||^2$$\n",
    "\n",
    "This means that maximizing the log-likelihood is equivalent to minimizing the squared error.\n",
    "\n",
    "$$\\min_{w} ||y - Xw||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4088c3",
   "metadata": {},
   "source": [
    "### Closed-form Solution\n",
    "\n",
    "We can find the formula for the weights that minimize the least squared error by solving for the closed-form solution. \n",
    "\n",
    "$$L(w) = ||y - Xw||^2 = y^T y - 2 w^T X^T y + w^T X^T X w$$\n",
    "\n",
    "So, we take the derivative of the loss.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dw} L(w) &= -2 X^T y + 2 X^T X w \\\\\n",
    "&= -2 X^T (y - Xw)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can set the derivative for $0$ and solve for $w$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-2 X^T (y - Xw) &= 0 \\\\\n",
    "X^T (y - Xw) &= 0 \\\\\n",
    "X^T y &= X^T Xw \\\\\n",
    "w &= (X^T X)^{-1} X^T y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, the least squares solution is the following.\n",
    "\n",
    "$$w = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6ce647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_mle(X, y):\n",
    "    \"\"\"\n",
    "    Linear regression closed-form solution (MLE under Gaussian noise)\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target vector of shape (n_samples,)\n",
    "\n",
    "    Returns:\n",
    "        Weight vector w that minimizes squared error, of shape (n_features,)\n",
    "    \"\"\"\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f7006",
   "metadata": {},
   "source": [
    "### Verifying using Gradient Descent\n",
    "\n",
    "We're trying to minimize the negative log-likelihood:\n",
    "\n",
    "$$L(w) = ||y - Xw||^2$$\n",
    "\n",
    "The gradient of the loss tells us the direction of steepest increase in loss, so moving in the opposite direction leads us toward lower loss.\n",
    "\n",
    "$$\\frac{d}{dw} L(w) = -2X^T(y - Xw)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92cb5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gradient(w, X, y):\n",
    "    \"\"\"\n",
    "        Computes the gradient of the squared loss for linear regression\n",
    "\n",
    "        Args:\n",
    "            w: Current weight vector of shape (n_features,)\n",
    "            X: Feature matrix of shape (n_samples, n_features)\n",
    "            y: Target vector of shape (n_samples,)\n",
    "\n",
    "        Returns:\n",
    "            Gradient vector of shape (n_features,)\n",
    "    \"\"\"\n",
    "    return -2 * X.T @ (y - X @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8429dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_true: [0.5 0.5]\n",
      "w_mle: [0.53015294 0.65566135]\n",
      "w_grad: [0.54363963 0.64144966]\n"
     ]
    }
   ],
   "source": [
    "# Fitting a plane\n",
    "n_features = 2\n",
    "\n",
    "# Generate true weights\n",
    "w_true = np.array([0.5, 0.5])  # np.random.rand(n_features)\n",
    "print('w_true:', w_true)\n",
    "\n",
    "# Generate random samples\n",
    "n_samples = 1000\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "\n",
    "# Generate noise (multiplying by sigma transforms N(0, 1) to N(0, sigma^2))\n",
    "sigma = 2\n",
    "epsilon = np.random.randn(n_samples) * sigma\n",
    "\n",
    "# Generate labels for samples\n",
    "y = X @ w_true + epsilon\n",
    "\n",
    "# Find estimated weights using closed-form MLE solution\n",
    "w_mle = linear_regression_mle(X, y)\n",
    "print('w_mle:', w_mle)\n",
    "\n",
    "# Verify closed-form solution using gradient descent\n",
    "w_grad = np.array([0.0, 0.0])\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.9\n",
    "for i in range(1000):\n",
    "    grad_vec = linear_regression_gradient(w_grad, X, y)\n",
    "    w_grad = w_grad - learning_rate * grad_vec\n",
    "    learning_rate *= learning_rate_decay\n",
    "print('w_grad:', w_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689e527",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression\n",
    "\n",
    "We're using Bayesian inference to:\n",
    "- Compute the posterior distribution over the weights given a prior distribution and likelihood\n",
    "- Sample from this posterior to visualize the predictive uncertainty\n",
    "- Show how confidence intervals shrink where data is dense and grow where data is sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe07ce5c",
   "metadata": {},
   "source": [
    "### Posterior Distribution\n",
    "\n",
    "For Bayesian inference, instead of treating $w$ as a variable, we treat it as a random variable. Let's assume that the prior over the weights is Gaussian.\n",
    "\n",
    "$$w \\sim \\mathcal{N}(0, \\sigma_w^2 I) \\propto \\exp(-\\frac{1}{2\\sigma_w^2}||w||^2)$$\n",
    "\n",
    "As in the MLE exercise, we're modeling a linear model with Gaussian noise so the likelihood is Gaussian conditioned on $X$ and $w$.\n",
    "\n",
    "$$y | X, w \\sim \\mathcal{N}(Xw, \\sigma^2 I) \\propto \\exp(-\\frac{1}{2\\sigma^2}||y - Xw||^2)$$\n",
    "\n",
    "Now that we have both the prior and likelihood, we can compute the posterior distribution using Bayes' Theorem.\n",
    "\n",
    "$$p(w | X, y) \\propto p(y | X, w) \\cdot p(w)$$\n",
    "\n",
    "There is a lengthy derivation on how to find resulting mean $\\mu_{post}$ and covariance $\\Sigma_{post}$ that starts with the following.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(w | X, y) &\\propto \\log p(y | X, w) + \\log p(w) \\\\\n",
    "&= -\\frac{1}{2\\sigma^2}(y - Xw)^T(y - Xw) - \\frac{1}{2\\sigma_w^2}w^T w + \\text{const}\n",
    "\\end{align}$$\n",
    "\n",
    "Eventually, we find that the posterior distribution is the following.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w | X, y &\\sim \\mathcal{N}(\\mu_{post}, \\Sigma_{post}) \\\\\n",
    "\\Sigma_{post} &= (\\frac{1}{\\sigma^2} X^T X + \\frac{1}{\\sigma_w^2} I)^{-1} \\\\\n",
    "\\mu_{post} &= \\Sigma_{post}(\\frac{1}{\\sigma^2}X^T y)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42687037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior_covariance(X, sigma, sigma_w):\n",
    "    \"\"\"\n",
    "    Computes the posterior covariance matrix of the weights in Bayesian linear regression.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        sigma: Standard deviation of the observation noise\n",
    "        sigma_w: Standard deviation of the prior over weights\n",
    "\n",
    "    Returns:\n",
    "        Posterior covariance matrix of shape (n_features, n_features)\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    return np.linalg.inv((1 / sigma**2) * X.T @ X + (1 / sigma_w**2) * np.eye(n_features))\n",
    "\n",
    "def compute_posterior_mean(X, y, sigma, sigma_w):\n",
    "    \"\"\"\n",
    "    Computes the posterior mean of the weights in Bayesian linear regression.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target vector of shape (n_samples,)\n",
    "        sigma: Standard deviation of the observation noise\n",
    "        sigma_w: Standard deviation of the prior over weights\n",
    "\n",
    "    Returns:\n",
    "        Posterior mean vector of shape (n_features,)\n",
    "    \"\"\"\n",
    "    covar = compute_posterior_covariance(X, sigma, sigma_w)\n",
    "    return covar @ ((1 / sigma**2) * X.T @ y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dad6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_true: [0.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.01010101],\n",
       "       [0.02020202],\n",
       "       [0.03030303],\n",
       "       [0.04040404],\n",
       "       [0.05050505],\n",
       "       [0.06060606],\n",
       "       [0.07070707],\n",
       "       [0.08080808],\n",
       "       [0.09090909],\n",
       "       [0.1010101 ],\n",
       "       [0.11111111],\n",
       "       [0.12121212],\n",
       "       [0.13131313],\n",
       "       [0.14141414],\n",
       "       [0.15151515],\n",
       "       [0.16161616],\n",
       "       [0.17171717],\n",
       "       [0.18181818],\n",
       "       [0.19191919],\n",
       "       [0.2020202 ],\n",
       "       [0.21212121],\n",
       "       [0.22222222],\n",
       "       [0.23232323],\n",
       "       [0.24242424],\n",
       "       [0.25252525],\n",
       "       [0.26262626],\n",
       "       [0.27272727],\n",
       "       [0.28282828],\n",
       "       [0.29292929],\n",
       "       [0.3030303 ],\n",
       "       [0.31313131],\n",
       "       [0.32323232],\n",
       "       [0.33333333],\n",
       "       [0.34343434],\n",
       "       [0.35353535],\n",
       "       [0.36363636],\n",
       "       [0.37373737],\n",
       "       [0.38383838],\n",
       "       [0.39393939],\n",
       "       [0.4040404 ],\n",
       "       [0.41414141],\n",
       "       [0.42424242],\n",
       "       [0.43434343],\n",
       "       [0.44444444],\n",
       "       [0.45454545],\n",
       "       [0.46464646],\n",
       "       [0.47474747],\n",
       "       [0.48484848],\n",
       "       [0.49494949],\n",
       "       [0.50505051],\n",
       "       [0.51515152],\n",
       "       [0.52525253],\n",
       "       [0.53535354],\n",
       "       [0.54545455],\n",
       "       [0.55555556],\n",
       "       [0.56565657],\n",
       "       [0.57575758],\n",
       "       [0.58585859],\n",
       "       [0.5959596 ],\n",
       "       [0.60606061],\n",
       "       [0.61616162],\n",
       "       [0.62626263],\n",
       "       [0.63636364],\n",
       "       [0.64646465],\n",
       "       [0.65656566],\n",
       "       [0.66666667],\n",
       "       [0.67676768],\n",
       "       [0.68686869],\n",
       "       [0.6969697 ],\n",
       "       [0.70707071],\n",
       "       [0.71717172],\n",
       "       [0.72727273],\n",
       "       [0.73737374],\n",
       "       [0.74747475],\n",
       "       [0.75757576],\n",
       "       [0.76767677],\n",
       "       [0.77777778],\n",
       "       [0.78787879],\n",
       "       [0.7979798 ],\n",
       "       [0.80808081],\n",
       "       [0.81818182],\n",
       "       [0.82828283],\n",
       "       [0.83838384],\n",
       "       [0.84848485],\n",
       "       [0.85858586],\n",
       "       [0.86868687],\n",
       "       [0.87878788],\n",
       "       [0.88888889],\n",
       "       [0.8989899 ],\n",
       "       [0.90909091],\n",
       "       [0.91919192],\n",
       "       [0.92929293],\n",
       "       [0.93939394],\n",
       "       [0.94949495],\n",
       "       [0.95959596],\n",
       "       [0.96969697],\n",
       "       [0.97979798],\n",
       "       [0.98989899],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a line\n",
    "n_features = 1\n",
    "\n",
    "# Generate true weights\n",
    "w_true = np.array([0.5])  # np.random.rand(n_features)\n",
    "print('w_true:', w_true)\n",
    "\n",
    "# Generate random samples\n",
    "n_samples = 1000\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "\n",
    "# Generate noise (multiplying by sigma transforms N(0, 1) to N(0, sigma^2))\n",
    "sigma = 2\n",
    "epsilon = np.random.randn(n_samples) * sigma\n",
    "\n",
    "# Generate labels for samples\n",
    "y = X @ w_true + epsilon\n",
    "\n",
    "# Assume a variance for the prior distribution\n",
    "sigma_w = 1.0\n",
    "\n",
    "# Generate a test grid\n",
    "X_test = np.linspace(0, 1, 100).reshape(-1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
